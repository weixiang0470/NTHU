{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages\n",
        "\n",
        "1. To build a recurrent neural network, we start by importing the Dense layer, Activation layer, and Loss function that you implemented in Lab4 & Flatten layer you implemented in Lab5. Ensure the following three files are located in the same directory as this notebook, and follow the instructions to complete the setup:\n",
        "    - Dense.py : Copy the **Dense class** you had implemented in Lab4 to it.\n",
        "    - Activation.py : Copy the **Activation class** you had implemented in Lab4 to it.\n",
        "    - Loss.py : Copy **compute_CCE_loss** function you had implemented in Lab4 to it.\n",
        "    - Flatten.py: Copy **Flatten class** you had implemented in Lab5 to it.\\\n",
        "    Note: you should copy both `forward()` and `backward()` in class `Flatten` in Lab5.\n",
        "\n",
        "\n",
        "⚠️ **WARNING** ⚠️:\n",
        "*   Please do not import any other packages in this lab.\n",
        "*   np.random.seed(seed) is used to keep all the random function calls consistent. It will help us grade your work. Please don't change the seed.\n",
        "\n",
        "❗ **Important** ❗: Please do not change the code outside this code bracket.\n",
        "```\n",
        "### START CODE HERE ###\n",
        "...\n",
        "### END CODE HERE ###\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "V7Zsk2Cyjcr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive (optional)"
      ],
      "metadata": {
        "id": "PHkPrxmpjkQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE HERE ###\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('YOUR PATH')\n",
        "### END CODE HERE ###"
      ],
      "metadata": {
        "id": "6-arb65yjiEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cd1c8d5-7cfa-4cc4-b3c7-4510f5a97cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### import your Lab4 & Lab5 code (Don't change this part) ######\n",
        "from Dense import Dense\n",
        "from Activation import Activation\n",
        "from Loss import compute_CCE_loss, compute_MSE_loss\n",
        "from Flatten import Flatten\n",
        "##################################\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "outputs = {}"
      ],
      "metadata": {
        "id": "poIfFIK0juEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Part"
      ],
      "metadata": {
        "id": "KAxzqsKx8bRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN Class (30%)\n",
        "The RNN class implements a simple Recurrent Neural Network (RNN) layer. This class includes methods for initializing parameters, performing the forward pass, computing gradients during the backward pass, and updating the weights.\n",
        "\n",
        "Structure overview:\n",
        "<figure> <img src=\"https://imgur.com/nU7xrBe.png\" width=\"580\" height=\"400\">\n",
        "</figure>\n",
        "RNN connection:\n",
        "<figure>\n",
        "<img src=\"https://imgur.com/4JQX0se.png\" width=\"500\" height=\"250\">\n",
        "</figure>\n",
        "Data insights:\n",
        "<figure> <img src=\"https://imgur.com/c2MI3mj.png\" width=\"400\" height=\"220\">\n",
        "</figure>\n",
        "In each neuron:\n",
        "<figure> <img src=\"https://imgur.com/luXS4zv.png\" width=\"300\" height=\"220\">\n",
        "</figure>\n",
        "\n",
        "1.   **Initializaon**\n",
        "\n",
        "    *   `input_size`: The number of input features for each time step.\n",
        "    *   `rnn_units`: The number of units (neurons) in the RNN layer.\n",
        "    *   `seed`: A random seed for weight initialization to ensure reproducibility.\n",
        "\n",
        "2.   **Initialize parameters** (3%)\n",
        "    * `Wx`: The weight matrix for the input to hidden connections. It has a shape of `(rnn_units, input_size)`\n",
        "    * `Wh`: The weight matrix for the hidden to hidden connections. It has a shape of `(rnn_units, rnn_units)`\n",
        "    * `bh`: The bias vector for the hidden state. It has a shape of `(rnn_units, 1)`, where rnn_units is the number of units in the RNN layer.\n",
        "3.  **Forward** (12%) \\\n",
        "* `X`: Input data of shape `(batch_size, timesteps, input_size)`\n",
        "* The forward pass computes the hidden state `h_t` (shape of `(batch_size, self.rnn_units)`)at each time step `t` using the following formula:\n",
        "$ h_t = \\tanh(W_x \\cdot x_t + W_h \\cdot h_{t-1} + b_h)$\n",
        "    * $W_x$ is the weight matrix for the input to hidden connections.\n",
        "    * $W_h$ is the weight matrix for the hidden to hidden connections.\n",
        "    * $b_h$ is the bias vector.\n",
        "    * $x_t$ is the input at time step t.\n",
        "    * $h_{t-1}$ is the hidden state from the previous time step.\n",
        "    * $\\tanh$ is the hyperbolic tangent activation function.\n",
        "* Output would be shape of `(batch_size, rnn_units)`\n",
        "4. **Backward** (15%) \\\n",
        "Reference (Backpropagation Through Time):\n",
        "https://www.pycodemates.com/2023/08/backpropagation-through-time-explained-with-derivations.html\n",
        "* `dH`: Gradient of the loss with respect to the hidden state, typically of shape `(batch_size, rnn_units, 1)`.\n",
        "* The backward pass computes the gradients of the loss with respect to the weights and biases using the following formulas:\n",
        "    * Gradient of the loss with respect to the hidden state(derivative of `tanh`):\\\n",
        "    $\\delta_t = (1 - h_t^2) \\cdot \\delta_{t+1}$\n",
        "        * $\\delta_t$ is the gradient of the loss with respect to the hidden state at time step t.\n",
        "        * $h_t$ is the hidden state at time step t\n",
        "    * Gradients with respect to the weights and biases (accumulate the gradients over all time steps):\n",
        "        1. $\\frac{\\partial L}{\\partial W_x} = ∑_{t=0}^{timesteps} \\delta_t^T \\cdot x_t$\n",
        "        2. $\\frac{\\partial L}{\\partial W_h} = ∑_ {t=0}^{timesteps} \\delta_t^T \\cdot h_{t-1}$\n",
        "        3. $\\frac{\\partial L}{\\partial b_h} = ∑_{t=0}^{timesteps} \\delta_t^T$\n",
        "        * $\\frac{\\partial L}{\\partial W_x}$ is the gradient of the loss with respect to the input to hidden weights.\n",
        "        * $\\frac{\\partial L}{\\partial W_h}$ is the gradient of the loss with respect to the hidden to hidden weights.\n",
        "        * $\\frac{\\partial L}{\\partial b_h}$ is the gradient of the loss with respect to the hidden bias.\n",
        "        * $x_t$ is the input at time step t.\n",
        "        * $h_{t-1}$ is the hidden state from the previous time step.\n",
        "    * Gradient with respect to the previous hidden state:\n",
        "    $\\frac{\\partial L}{\\partial h_{t-1}} = \\delta_t \\cdot W_h$\n",
        "    * Then divide each gradient by `batch_size`.\n",
        "    * Gradient clipping (optional) is recommended in RNN since RNN rely on backpropagation through time where data might contain large timesteps and it might cause gradient explosion or vanishing. **Note**: You can't do gradient clipping in the function testing (`test_baward()`)\n"
      ],
      "metadata": {
        "id": "PhkjTLnlPNjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN():\n",
        "    def __init__(self, input_size, rnn_units, seed=1):\n",
        "        \"\"\"\n",
        "        Initialize the SimpleRNN layer.\n",
        "\n",
        "        Parameters:\n",
        "        input_size (int): Number of input features.\n",
        "        rnn_units (int): Number of units in the RNN layer.\n",
        "        seed (int): Random seed for weight initialization.\n",
        "        \"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.rnn_units = rnn_units\n",
        "        self.seed = seed\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\"\n",
        "        Initialize the weights and biases for the RNN layer.\n",
        "        \"\"\"\n",
        "        np.random.seed(self.seed)\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "        # Multiply by 0.01 is for testing reason.\n",
        "        self.Wx = np.random.randn(None) * 0.01     # Input to hidden weights , with shape of (rnn_units, input_size).\n",
        "        self.Wh = np.random.randn(None) * 0.01     # Hidden to hidden weights, with shape of (rnn_units, rnn_units).\n",
        "        self.bh = np.zeros(None)     # Hidden bias, with shape of (rnn_units, 1).\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Perform the forward pass through the RNN layer.\n",
        "\n",
        "        Parameters:\n",
        "        X (ndarray): Input data of shape (batch_size, timesteps, input_size).\n",
        "\n",
        "        Returns:\n",
        "        ndarray: Output of the RNN layer (hidden state).\n",
        "        \"\"\"\n",
        "        batch_size, timesteps, _ = X.shape\n",
        "        ### START CODE HERE ###\n",
        "        self.h = None       # Initialize hidden state, with shape of (batch_size, self.rnn_units).\n",
        "        self.hs = None      # Store hidden states for backward pass, list of self.h.\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        self.xs = []  # Store inputs for backward pass\n",
        "\n",
        "        for t in range(timesteps):\n",
        "            ### START CODE HERE ###\n",
        "            x_t = None              # Get input at time step t for all data in X.\n",
        "            self.xs.append(x_t)\n",
        "            self.h = None           # Update hidden state according to the formula of h_t.\n",
        "            self.hs.append(self.h)\n",
        "            ### END CODE HERE ###\n",
        "\n",
        "        return self.h\n",
        "\n",
        "\n",
        "    def backward(self, dH):\n",
        "        \"\"\"\n",
        "        Perform the backward pass through the RNN layer.\n",
        "\n",
        "        Parameters:\n",
        "        dH (ndarray): Gradient of the loss with respect to the hidden state.\n",
        "        clip_value (float): Value to clip the gradients to prevent exploding gradients.\n",
        "\n",
        "        Returns:\n",
        "        ndarray: Gradient of the loss with respect to the input.\n",
        "        \"\"\"\n",
        "        batch_size, _ = dH.shape\n",
        "        ### START CODE HERE ###\n",
        "        self.dL_dWx = None        # Initialized with same shape as Wx.\n",
        "        self.dL_dWh = None        # Initialized with same shape as Wh.\n",
        "        self.dL_dbh = None        # Initialized with same shape as bh.\n",
        "        ### END CODE HERE ###\n",
        "        dL_dh = dH\n",
        "\n",
        "        for t in reversed(range(len(self.hs))):\n",
        "            x_t = self.xs[t]  # Get input at time step t\n",
        "            h = self.hs[t]\n",
        "            h_prev = self.hs[t-1] if t > 0 else np.zeros_like(h)\n",
        "            ### START CODE HERE ###\n",
        "            dL_dh_raw = None     # Derivative of tanh\n",
        "            self.dL_dWx = None   # Gradient w.r.t. Wx\n",
        "            self.dL_dWh = None   # Gradient w.r.t. Wh\n",
        "            self.dL_dbh = None   # Gradient w.r.t. bh\n",
        "            dL_dh = None         # Gradient w.r.t. previous hidden state (Wh)\n",
        "            ### END CODE HERE ###\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "        # Average gradients over the batch.\n",
        "        self.dL_dWx = None\n",
        "        self.dL_dWh = None\n",
        "        self.dL_dbh = None\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "        # Gradient part (optional), you can't run this part while testing the backward function !!!\n",
        "        # clip_value = 2.0\n",
        "        # Clip gradients to prevent exploding gradients\n",
        "        # np.clip(self.dL_dWx, -clip_value, clip_value, out=self.dL_dWx)\n",
        "        # np.clip(self.dL_dWh, -clip_value, clip_value, out=self.dL_dWh)\n",
        "        # np.clip(self.dL_dbh, -clip_value, clip_value, out=self.dL_dbh)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        return dL_dh\n",
        "\n",
        "    def update(self, learning_rate):\n",
        "        \"\"\"\n",
        "        Update the weights and biases using the computed gradients.\n",
        "\n",
        "        Parameters:\n",
        "        learning_rate (float): Learning rate for weight updates.\n",
        "        \"\"\"\n",
        "        ### START CODE HERE ###\n",
        "        # Update each weights.\n",
        "        self.Wx = None\n",
        "        self.Wh = None\n",
        "        self.bh = None\n",
        "        ### END CODE HERE ###"
      ],
      "metadata": {
        "id": "Ry3gZ4hUJwUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions testing\n"
      ],
      "metadata": {
        "id": "ZkQSHtRPMb73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `initialize_parameter` function"
      ],
      "metadata": {
        "id": "KKZFZhPfP9z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_initialization(save_to_output = False):\n",
        "    input_size = 5\n",
        "    rnn_units = 3\n",
        "    rnn = RNN(input_size, rnn_units)\n",
        "    print(\"Wx shape:\", rnn.Wx.shape)\n",
        "    print(\"Wh shape:\", rnn.Wh.shape)\n",
        "    print(\"bh shape:\", rnn.bh.shape)\n",
        "    if save_to_output == True:\n",
        "        outputs['RNN_initialization'] = {\"Wx shape\": rnn.Wx.shape, \"Wh shape\": rnn.Wh.shape, \"bh shape\": rnn.bh.shape}\n",
        "test_initialization(True)"
      ],
      "metadata": {
        "id": "F6x4VR14P9X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output:\n",
        "\n",
        "|Output|Value|\n",
        "|---|---|\n",
        "|Wx shape:|(3, 5)|\n",
        "|Wh shape:|(3, 3)|\n",
        "|bh shape:|(3, 1)|"
      ],
      "metadata": {
        "id": "TPEe5c4NQigx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `forward` function\n"
      ],
      "metadata": {
        "id": "Oz_VET-YNtla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_forward(seed, save_to_output = False):\n",
        "    input_size = 5\n",
        "    rnn_units = 3\n",
        "    batch_size = 2\n",
        "    timesteps = 4\n",
        "    # Initialize the RNN layer\n",
        "    rnn = RNN(input_size, rnn_units)\n",
        "    # Create a sample input of shape (batch_size, timesteps, input_size)\n",
        "    np.random.seed(seed)   # Don't change the seed!\n",
        "    X = np.random.randn(batch_size, timesteps, input_size)\n",
        "    # Perform the forward pass\n",
        "    output = rnn.forward(X)\n",
        "\n",
        "    if save_to_output == False:\n",
        "        # Print the output\n",
        "        print(\"Input shape:\", X.shape)\n",
        "        print(\"Input:\")\n",
        "        print(X)\n",
        "        print(\"Output shape of RNN layer:\", output.shape)\n",
        "        print(\"Output of the RNN layer (hidden state):\")\n",
        "        print(output)\n",
        "\n",
        "    if save_to_output == True:\n",
        "        outputs['RNN_forward'] = {\"X shape\": X.shape, \"X\": X, \"Output shape\": output.shape, \"Output\": output}\n",
        "\n",
        "# Run the test with seed 1 to compare with expected outputs.\n",
        "test_forward(1)\n",
        "\n",
        "# Run with seed 42 and save the answer.\n",
        "test_forward(42, True)"
      ],
      "metadata": {
        "id": "-7svLiJmFdEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output of `forward`:\n",
        "\n",
        "| Output | Value |\n",
        "|---|---|\n",
        "| Input shape | (2, 4, 5) |\n",
        "| Input | `[[[ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763]`<br>  `[-2.3015387   1.74481176 -0.7612069   0.3190391  -0.24937038]`<br>  `[ 1.46210794 -2.06014071 -0.3224172  -0.38405435  1.13376944]`<br>  `[-1.09989127 -0.17242821 -0.87785842  0.04221375  0.58281521]]`<br><br> `[[-1.10061918  1.14472371  0.90159072  0.50249434  0.90085595]`<br>  `[-0.68372786 -0.12289023 -0.93576943 -0.26788808  0.53035547]`<br>  `[-0.69166075 -0.39675353 -0.6871727  -0.84520564 -0.67124613]`<br>  `[-0.0126646  -1.11731035  0.2344157   1.65980218  0.74204416]]]]` |\n",
        "| Output shape of RNN layer | (2, 3) |\n",
        "| Output of the RNN layer (hidden state) | `[[-0.00873835  0.02641052 -0.00287992]`<br> `[-0.00596289 -0.01742189  0.02418844]]` |"
      ],
      "metadata": {
        "id": "GxLx95MZGEqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `backward` function"
      ],
      "metadata": {
        "id": "OTRcUEWzN143"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_backward(seed, save_to_output = False):\n",
        "    input_size = 5\n",
        "    rnn_units = 3\n",
        "    batch_size = 2\n",
        "    timesteps = 4\n",
        "\n",
        "    # Initialize the RNN layer\n",
        "    rnn = RNN(input_size, rnn_units)\n",
        "    np.random.seed(seed)   # Don't change the seed!\n",
        "    # Create a sample input of shape (batch_size, timesteps, input_size)\n",
        "    X = np.random.randn(batch_size, timesteps, input_size)\n",
        "    # Perform the forward pass\n",
        "    rnn.forward(X)\n",
        "    # Create a sample gradient of the loss with respect to the hidden state\n",
        "    dH = np.random.randn(batch_size, rnn_units)\n",
        "    # Perform the backward pass\n",
        "    dL_dX = rnn.backward(dH)\n",
        "    if save_to_output == False:\n",
        "        print(\"dH shape: \", dH.shape)\n",
        "        print(\"dH:\")\n",
        "        print(dH)\n",
        "        # Print the gradients\n",
        "        print(\"dL_dX shape:\", dL_dX.shape)\n",
        "        print(\"Gradient w.r.t. input (dL_dX):\")\n",
        "        print(dL_dX)\n",
        "        print(\"dL_dWx shape:\", rnn.dL_dWx.shape)\n",
        "        print(\"Gradient w.r.t. Wx (dL_dWx):\")\n",
        "        print(rnn.dL_dWx)\n",
        "        print(\"dL_dWh shape:\", rnn.dL_dWh.shape)\n",
        "        print(\"Gradient w.r.t. Wh (dL_dWh):\")\n",
        "        print(rnn.dL_dWh)\n",
        "        print(\"dL_dbh shape:\", rnn.dL_dbh.shape)\n",
        "        print(\"Gradient w.r.t. bh (dL_dbh):\")\n",
        "        print(rnn.dL_dbh)\n",
        "\n",
        "    if save_to_output == True:\n",
        "        outputs['RNN_backward'] = {\"dH shape\": dH.shape, \"dH\": dH, \"dL_dX shape\": dL_dX.shape, \"dL_dX\": dL_dX, \"dL_dWx shape\": rnn.dL_dWx.shape,\n",
        "                                   \"dL_dWx\": rnn.dL_dWx, \"dL_dWh shape\": rnn.dL_dWh.shape, \"dL_dWh\": rnn.dL_dWh, \"dL_dbh shape\": rnn.dL_dbh.shape, \"dL_dbh\": rnn.dL_dbh}\n",
        "\n",
        "# Run the test with seed 1 to compare with expected outputs.\n",
        "test_backward(1)\n",
        "\n",
        "# Run with seed 42 and save the answer.\n",
        "test_backward(42, True)"
      ],
      "metadata": {
        "id": "hc5rkLYhJHYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output:\n",
        "\n",
        "| Output | Value |\n",
        "|---|---|\n",
        "|dH shape:|(2, 3)|\n",
        "|dH:|`[[-0.19183555 -0.88762896 -0.74715829]`<br>`[ 1.6924546   0.05080775 -0.63699565]]`|\n",
        "|dL_dX shape:| (2, 3)|\n",
        "|Gradient w.r.t. input (dL_dX):| `[[-1.30293142e-08  5.55316892e-09 -2.93987480e-08]`<br>`[ 8.78019041e-09  2.41595285e-08 -1.33357681e-08]]`|\n",
        "|dL_dWx shape:| (3, 5)|\n",
        "|Gradient w.r.t. Wx (dL_dWx):|`[[ 0.09855041 -0.91665683  0.29245483  1.41272981  0.57684949]`<br>`[ 0.48200032  0.06164319  0.40009246  0.02920006 -0.24340623]`<br>`[ 0.42661386  0.4160692   0.25830352 -0.53769917 -0.44331323]]`|\n",
        "|dL_dWh shape:| (3, 3)|\n",
        "|Gradient w.r.t. Wh (dL_dWh):|`[[-0.00663169  0.01738478 -0.0106337 ]`<br>`[-0.02325593  0.03069429 -0.03469078]`<br>`[-0.01936184  0.02200992 -0.02853306]]`|\n",
        "|dL_dbh shape:| (3, 1)|\n",
        "|Gradient w.r.t. bh (dL_dbh):|`[[ 0.73406116]`<br>`[-0.42812618]`<br>`[-0.69714697]]`|"
      ],
      "metadata": {
        "id": "l0rtq6lDJSx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model class (10%)\n",
        "The `Model` class provides a framework for building and training neural networks.\n",
        "1.   `add()` is just like the function in previous Lab, we can call `model.add( )` to sequentially add a layer into the model.\n",
        "2.   `forward()` performs the forward propagation of data through the layers, calculating predictions\n",
        "3.   In `backward()`, we have to check if the activation function is *softmax* to make sure we call the right `backward()`.\n",
        "4.   `train()` function will be used to train the model, here, we can decide which loss function `BCE` or `MSE` we want to train with. And also calculate the loss for validation data.\n",
        "5. `plot_losses()` function can show the history of the training and validation loss to see if the training works."
      ],
      "metadata": {
        "id": "3_--VLgVCdpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model():\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "    def add(self, layer):\n",
        "        \"\"\"\n",
        "        Sequentially add a layer into the model.\n",
        "\n",
        "        Parameters:\n",
        "        layer: Different layers class.\n",
        "        \"\"\"\n",
        "        ### START CODE HERE ###\n",
        "        None\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def forward(self, X):\n",
        "        for layer in self.layers:\n",
        "            if layer.__class__.__name__ == 'RNN':\n",
        "                if len(X.shape) < 3:                                # If the RNN layer is stacked after another RNN layer.\n",
        "                    ### START CODE HERE ###\n",
        "                    X = None                # We have to treat the output size of first layer (rnn_units) as the feature size (input_size) of the second layer.\n",
        "                    ### END CODE HERE ###\n",
        "                X = layer.forward(X)\n",
        "            else:\n",
        "                X = layer.forward(X)\n",
        "        return X\n",
        "\n",
        "    def backward(self, dA, Y):\n",
        "        for layer in reversed(self.layers):\n",
        "            if isinstance(layer, Activation) and layer.activation_function == \"softmax\":\n",
        "                ### START CODE HERE ###\n",
        "                dA = None                   # softmax activation backward.\n",
        "                ### END CODE HERE ###\n",
        "            else:\n",
        "                ### START CODE HERE ###\n",
        "                dA = None\n",
        "                ### END CODE HERE ###\n",
        "        return dA\n",
        "\n",
        "    def update(self, learning_rate):\n",
        "        for layer in self.layers:       # Update for every layers.\n",
        "            ### START CODE HERE ###\n",
        "            None\n",
        "            ### END CODE HERE ###\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs=10, learning_rate=0.001, batch_size = 32, loss_function='mse'):\n",
        "        self.train_losses = []  # Initialize a list to store training losses\n",
        "        self.val_losses = []  # Initialize a list to store validation losses\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            ### START CODE HERE ###\n",
        "            num_batches = None    # Calculate the number of batches\n",
        "            ### END CODE HERE ###\n",
        "\n",
        "            with tqdm(total=num_batches, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\") as pbar:\n",
        "                for batch_idx in range(num_batches):\n",
        "\n",
        "                    ### START CODE HERE ###\n",
        "                    # Get the batch data\n",
        "                    X_batch = None\n",
        "                    y_batch = None\n",
        "                    ### END CODE HERE ###\n",
        "\n",
        "                    ### START CODE HERE ###\n",
        "                    # 1. Forward to get the prediction.\n",
        "                    # 2. Calculate the loss according to your parameters ('cce' or 'mse').\n",
        "                    # 3. Calculate dA.\n",
        "                    # 4. backward with the calculated dA.\n",
        "                    # 5. update the parameters.\n",
        "                    y_pred = None\n",
        "                    # Compute loss\n",
        "                    if loss_function == 'cce':\n",
        "                        loss = None\n",
        "                    elif loss_function == 'mse':\n",
        "                        loss = None\n",
        "                    else:\n",
        "                        raise ValueError(\"Unsupported loss function\")\n",
        "\n",
        "                    total_loss += loss\n",
        "                    dA = None\n",
        "                    None\n",
        "                    ### END CODE HERE ###\n",
        "\n",
        "                    # Update the progress bar and loss every 5 iterations\n",
        "                    if (batch_idx + 1) % 5 == 0:\n",
        "                        pbar.set_postfix(loss=total_loss / (batch_idx + 1))\n",
        "                    pbar.update(1)  # Increment the progress bar by 1 unit\n",
        "\n",
        "                # Handle the remaining examples that do not fit into a full batch\n",
        "                if len(X_train) % batch_size != 0:\n",
        "\n",
        "                    # Get the remaining data\n",
        "                    ### START CODE HERE ###\n",
        "                    X_batch = None\n",
        "                    y_batch = None\n",
        "                    ### END CODE HERE ###\n",
        "\n",
        "                    ### START CODE HERE ###\n",
        "                    # Same as above in batch\n",
        "                    y_pred = None\n",
        "                    # Compute loss\n",
        "                    if loss_function == 'cce':\n",
        "                        loss = None\n",
        "                    elif loss_function == 'mse':\n",
        "                        loss = None\n",
        "                    else:\n",
        "                        raise ValueError(\"Unsupported loss function\")\n",
        "\n",
        "                    total_loss += loss\n",
        "                    dA = None\n",
        "                    None\n",
        "                    ### END CODE HERE ###\n",
        "\n",
        "            ### START CODE HERE ###\n",
        "            avg_train_loss = None       # Calculate the average loss over batches.\n",
        "            ### END CODE HERE ###\n",
        "\n",
        "            self.train_losses.append(avg_train_loss)\n",
        "            print(f'Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss}')\n",
        "\n",
        "            # Validation part\n",
        "            ### START CODE HERE ###\n",
        "            # 1. Get the prediction\n",
        "            # 2. compute the loss ('mse', 'cce').\n",
        "            y_pred = None\n",
        "\n",
        "            if loss_function == 'cce':\n",
        "                val_loss = None\n",
        "            elif loss_function == 'mse':\n",
        "                val_loss = None\n",
        "            ### END CODE HERE ###\n",
        "\n",
        "            self.val_losses.append(val_loss)\n",
        "            print(f'Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "    def plot_losses(self):\n",
        "        plt.plot(self.train_losses, label='Training Loss')\n",
        "        plt.plot(self.val_losses, label='Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()"
      ],
      "metadata": {
        "id": "LlyznHZjCc0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Model` class testing"
      ],
      "metadata": {
        "id": "icEnOl_kR_r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_class(seed, save_to_output = False):\n",
        "    np.random.seed(seed)\n",
        "    input = np.random.randn(3, 3, 1)\n",
        "    Y = np.expand_dims(np.array([1,0,1]),-1)\n",
        "\n",
        "    model = Model()\n",
        "    model.add(RNN(1, 4))\n",
        "    model.add(RNN(4, 4))\n",
        "    model.add(Dense(4, 1))\n",
        "    model.add(Activation(\"sigmoid\", None))\n",
        "\n",
        "    AL = model.forward(input)\n",
        "    dA_prev = model.backward(AL, Y=Y)\n",
        "    model.update(0.1)\n",
        "    if save_to_output == False:\n",
        "        print(\"AL: \", AL)\n",
        "        print(\"dA_prev: \", dA_prev)\n",
        "        print(\"Wx of RNN: \", model.layers[0].Wx)\n",
        "        print(\"Wh of RNN: \", model.layers[0].Wh)\n",
        "        print(\"bh of RNN: \", model.layers[0].bh)\n",
        "\n",
        "    if save_to_output == True:\n",
        "        outputs['Model_class'] = {\"AL\": AL, \"dA_prev\": dA_prev, \"Wx of RNN\": model.layers[0].Wx, \"Wh of RNN\": model.layers[0].Wh, \"bh of RNN\": model.layers[0].bh}\n",
        "\n",
        "# Run the test with seed 1 to compare with expected outputs.\n",
        "test_model_class(1)\n",
        "\n",
        "# Run with seed 42 and save the answer.\n",
        "test_model_class(42, True)"
      ],
      "metadata": {
        "id": "B2ON5fmPSCSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output:\n",
        "\n",
        "|Output|Value|\n",
        "|---|---|\n",
        "|AL:| `[[0.50002597]`<br>`[0.50011082]`<br>`[0.49998505]]`|\n",
        "|dA_prev:|`[[ 1.62378331e-09  3.84694676e-09  6.02625343e-09 -1.21805669e-08]`<br>`[ 1.62165475e-09  3.84752280e-09  6.02247634e-09 -1.21748332e-08]`<br>`[ 1.62368851e-09  3.84622277e-09  6.02587086e-09 -1.21793784e-08]]`|\n",
        "|Wx of RNN:| `[[ 0.01609996]`<br>`[-0.00598851]`<br>`[-0.00519148]`<br>`[-0.01059058]]`|\n",
        "|Wh of RNN:|`[[ 0.00865366 -0.02301519  0.01744827 -0.00761178]`<br>`[ 0.00319076 -0.00249388  0.01462095 -0.02060167]`<br>`[-0.00322391 -0.00384067  0.0113376  -0.0109991 ]`<br>`[-0.00172383 -0.00877879  0.00042198  0.00582784]]`|\n",
        "|bh of RNN:|  `[[ 0.00017288]`<br>`[-0.00015564]`<br>`[-0.00010827]`<br>`[-0.00016452]]`|"
      ],
      "metadata": {
        "id": "Seg7OO12VOvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the above output to `.npy`"
      ],
      "metadata": {
        "id": "gEMEe8JMkmhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert list(outputs.keys()) == ['RNN_initialization', 'RNN_forward', 'RNN_backward', 'Model_class']\n",
        "np.save('Lab6_outputs.npy', outputs)\n",
        "# sanity check for saved outputs\n",
        "submit = np.load(\"Lab6_outputs.npy\", allow_pickle=True).item()\n",
        "for key, value in submit.items():\n",
        "    if isinstance(value, dict):  # Check if value is a dictionary\n",
        "        print(f\"{key}:\")\n",
        "        for inner_key, inner_value in value.items():\n",
        "            print(f\"  {inner_key}: {type(inner_value)}\")  # Print type of inner values\n",
        "    else:\n",
        "        print(f\"{key}: {type(value)}\")  # Print type of other values"
      ],
      "metadata": {
        "id": "W6SUWskHkmMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output:\n",
        "\n",
        "\n",
        "```\n",
        "RNN_initialization:\n",
        "  Wx shape: <class 'tuple'>\n",
        "  Wh shape: <class 'tuple'>\n",
        "  bh shape: <class 'tuple'>\n",
        "RNN_forward:\n",
        "  X shape: <class 'tuple'>\n",
        "  X: <class 'numpy.ndarray'>\n",
        "  Output shape: <class 'tuple'>\n",
        "  Output: <class 'numpy.ndarray'>\n",
        "RNN_backward:\n",
        "  dH shape: <class 'tuple'>\n",
        "  dH: <class 'numpy.ndarray'>\n",
        "  dL_dX shape: <class 'tuple'>\n",
        "  dL_dX: <class 'numpy.ndarray'>\n",
        "  dL_dWx shape: <class 'tuple'>\n",
        "  dL_dWx: <class 'numpy.ndarray'>\n",
        "  dL_dWh shape: <class 'tuple'>\n",
        "  dL_dWh: <class 'numpy.ndarray'>\n",
        "  dL_dbh shape: <class 'tuple'>\n",
        "  dL_dbh: <class 'numpy.ndarray'>\n",
        "Model_class:\n",
        "  AL: <class 'numpy.ndarray'>\n",
        "  dA_prev: <class 'numpy.ndarray'>\n",
        "  Wx of RNN: <class 'numpy.ndarray'>\n",
        "  Wh of RNN: <class 'numpy.ndarray'>\n",
        "  bh of RNN: <class 'numpy.ndarray'>\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "m-Q5MP76mda2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sinusoidal wave dataset (20%)\n",
        "In this section, we will generate a dataset of sinusoidal waves with varying frequencies, amplitudes, and random phase shifts. The **last value** in each sequence will be used as the target prediction. This dataset will be used for training and validating a model.\n",
        "\n",
        "\n",
        "\n",
        "*   You can decide how many `num_samples`, `num_timesteps`, `freq_range` and `amp_range` you want to generate your own training dataset. (**there is no provided training dataset!**)\n",
        "* ⚠⚠ You need to download X_test.csv from Kaggle and put it into Sinewave directory. (Or you have to change the path when generating prediction)\n",
        "* For **testing data** on Kaggle, we used `num_timesteps = 100`, `0.5 <= freq_range <= 5` & `0.5 <= amp_range <= 5` to generate, your training dataset can try to cover these range in testing data.\n",
        "* You need submit the `y_test.csv` to Kaggle\n",
        "    * `MAPE <= 17%` -> 10 points\n",
        "    * `MAPE <= 14%` -> 20 points\n",
        "* **Kaggle Link**: https://www.kaggle.com/t/512f44fe285d4c1bb90c39884f8a2a33"
      ],
      "metadata": {
        "id": "QOLxLMMl9BD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Sinusoidal Wave Dataset\n",
        "\n",
        "def generate_sine_wave_data(num_samples, num_timesteps, freq_range, amp_range):\n",
        "    \"\"\"\n",
        "    Generate sine wave data with varying frequencies, amplitudes, and random phase shifts.\n",
        "    The last value in each sequence is used as the target prediction.\n",
        "\n",
        "    Parameters:\n",
        "    - num_samples: Number of samples to generate.\n",
        "    - num_timesteps: Number of timesteps for each sample.\n",
        "    - freq_range: Tuple of floor and ceiling of frequency range.\n",
        "    - amp_range: Tuple of floor and ceiling of amplitude range.\n",
        "\n",
        "    Returns:\n",
        "    - X: Generated sine wave data of shape (num_samples, num_timesteps - 1).\n",
        "    - y: Target values of shape (num_samples,).\n",
        "    \"\"\"\n",
        "    X = np.zeros((num_samples, num_timesteps - 1))\n",
        "    y = np.zeros(num_samples)\n",
        "    for i in range(num_samples):\n",
        "        ### START CODE HERE ###\n",
        "        # Choose the frequency, amplitude and shift phase value.\n",
        "        freq = None             # Random choose from freq_range.\n",
        "        amp = None              # Random choose from amp_range.\n",
        "        phase_shift = None      # Random choose from (0, 2*pi).\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        t = np.linspace(0, 2 * np.pi, num_timesteps)\n",
        "        sine_wave = amp * np.sin(freq * t + phase_shift)\n",
        "        X[i] = sine_wave[:-1]  # All but the last value\n",
        "        y[i] = sine_wave[-1]   # The last value\n",
        "    return X, y\n",
        "\n",
        "### START CODE HERE ###\n",
        "# You can modify to your preferred range and number of samples. (Note: It's recommended to cover the range of testing data mentioned above.)\n",
        "num_samples = 1600\n",
        "num_timesteps = 100\n",
        "freq_range = (0.5, 10.0)  # Frequency range\n",
        "amp_range = (0.5, 10.0)   # Amplitude range\n",
        "### END CODE HERE ###\n",
        "\n",
        "X, y = generate_sine_wave_data(num_samples, num_timesteps, freq_range, amp_range)\n",
        "\n",
        "### START CODE HERE ###\n",
        "# Split data into training and validation sets\n",
        "None\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Plot an example sequence\n",
        "plt.plot(X_train[0], label='Input Sequence')\n",
        "plt.plot(np.arange(num_timesteps, num_timesteps + 1), y_train[0], 'ro', label='Target Value')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4eIHk7hbB7Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construct the model with Dense layer only\n",
        "Here, we want you to construct the model with **Dense layer only** like you did in Lab4. You may modify the model structure and parameters in the way you like."
      ],
      "metadata": {
        "id": "ambyK8w0CYko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE HERE ###\n",
        "# Change the structure and parameters to train your own model (Only contain Dense layer here!!!)\n",
        "# Reshape the input data to fit Dense model\n",
        "input_size = X_train.shape[1]\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1])\n",
        "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1])\n",
        "y_val = y_val.reshape(-1, 1)\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "# Construct the model with Dense layers\n",
        "\n",
        "# model = Model()\n",
        "# model.add(Dense(input_size, 64))\n",
        "# model.add(Activation(\"relu\", None))\n",
        "# model.add(Dense(64, 1))\n",
        "# model.add(Activation(\"linear\", None))\n",
        "\n",
        "# Train the model\n",
        "model.train(X_train, y_train, X_val, y_val, epochs=20, learning_rate=0.001, batch_size = 8, loss_function='mse')\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Plot the training and validation losses\n",
        "model.plot_losses()"
      ],
      "metadata": {
        "id": "NHebwBbOCeZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict testing data & Save the answer (For Dense model)\n",
        "For this prediction of Dense model, you can choose not to submit it to Kaggle if its performance is not better."
      ],
      "metadata": {
        "id": "xP0-p2yrD8gA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE HERE ###\n",
        "# Read the X_test.csv file into a DataFrame\n",
        "# Change the path if needed\n",
        "# download\n",
        "X_test_df = pd.read_csv('Sinewave/X_test.csv')\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Do not modify this part to get the correct output format!!\n",
        "# Drop the 'Id' column if it exists\n",
        "if 'Id' in X_test_df.columns:\n",
        "    X_test_df = X_test_df.drop(columns=['Id'])\n",
        "\n",
        "# Convert the DataFrame to a numpy array\n",
        "X_test = X_test_df.values\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "# Convert the list of predictions to a numpy array\n",
        "y_pred = np.array(y_pred)\n",
        "y_pred_df = pd.DataFrame(y_pred, columns=['answer'])\n",
        "y_pred_df.insert(0, 'Id', range(1, 1 + len(y_pred_df)))\n",
        "y_pred_df.to_csv('y_pred_basic.csv', index=False)\n",
        "\n",
        "print('Prediction data has been saved.')"
      ],
      "metadata": {
        "id": "_GNngIotD6Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construct the model with RNN layer\n",
        "Then, we can integrate our custom-built RNN layer into the model to evaluate whether it improves performance on the sequential inputs."
      ],
      "metadata": {
        "id": "0E7dvPVBBhgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE HERE ###\n",
        "# Change the structure and parameters to train your own model (Can add RNN layer here)\n",
        "# Reshape X_train and X_val to fit the RNN layer input shape.\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
        "y_val = y_val.reshape(-1, 1)\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "\n",
        "# Construct the model & Set the parameters.\n",
        "input_size = 1  # Number of input features (1 for single sine wave value)\n",
        "rnn_units = 32  # Number of units in the RNN layer\n",
        "dense_units = 16  # Number of units in the Dense layer\n",
        "output_size = 1  # Number of output classes (1 for single sine wave value)\n",
        "\n",
        "# model = Model()\n",
        "model.add(None)\n",
        "...\n",
        "\n",
        "# Train the model\n",
        "model.train(X_train, y_train, X_val, y_val, epochs=20, learning_rate=0.001, batch_size = 16, loss_function='mse')\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Plot the training and validation losses\n",
        "model.plot_losses()"
      ],
      "metadata": {
        "id": "yIJd6igNCG8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict testing data & Save the answer (For RNN model)\n",
        "Remember to submit your prediction to Kaggle!"
      ],
      "metadata": {
        "id": "DafczhXYA0fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE HERE ###\n",
        "# Read the X_test.csv file into a DataFrame\n",
        "# Change the path if needed\n",
        "X_test_df = pd.read_csv('Sinewave/X_test.csv')\n",
        "### END CODE HERE ###\n",
        "\n",
        "\n",
        "# Do not modify this part to get the correct output format!!\n",
        "# Drop the 'Id' column if it exists\n",
        "if 'Id' in X_test_df.columns:\n",
        "    X_test_df = X_test_df.drop(columns=['Id'])\n",
        "\n",
        "# Convert the DataFrame to a numpy array\n",
        "X_test = X_test_df.values\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))  # reshape the X_test to fit RNN layer input shape.\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "# Convert the list of predictions to a numpy array\n",
        "y_pred = np.array(y_pred)\n",
        "y_pred_df = pd.DataFrame(y_pred, columns=['answer'])\n",
        "y_pred_df.insert(0, 'Id', range(1, 1 + len(y_pred_df)))\n",
        "y_pred_df.to_csv('y_pred_basic.csv', index=False)\n",
        "\n",
        "print('Prediction data has been saved.')"
      ],
      "metadata": {
        "id": "kVkkIBwd-CWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advance part (35%)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NNRANy31n8aW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accelerometer and Gyroscope dataset\n",
        "- ⚠⚠ You need to download the training & testing data from Kaggle. ⚠⚠ (Put it into the directory name `Activity data`, or your need to change the path in template.)\n",
        "- In this dataset, we provide the time-series Accelerometer and Gyroscope data each with 3-axial (x, y, z) (total 6 channels).\n",
        "- We classified the activity into 3 different classes:\n",
        "    Activity 1, 2, 3\\\n",
        "    You have to build a RNN model using these time-series data to predict the activity class.\n",
        "- Steps:\n",
        "    1. Load the provided `X_train.npy`, `y_train.npy` and `X_test.npy` and split the part of training data to validation.\\\n",
        "    Note: `y_train.npy` is already one-hot encoded.\n",
        "    2. Visualize the `X_train.npy`.\n",
        "    3. One-hot encode the `y_train.npy` for three classes: Activity 1, Activity 2, Activity 3.\n",
        "    4. Build your own RNN model and train it.\n",
        "    5. Predict with the `X_test.npy` and generate `y_test.csv` then **submit it to Kaggle**!\n",
        "- We have set 3 baselines on public score:\n",
        "    * Accuracy >= 0.65 -> 10 points\n",
        "    * Accuracy >= 0.7  -> 20 points\n",
        "    * Accuracy >= 0.75 -> 25 points\n",
        "- Kaggle link: https://www.kaggle.com/t/95e0b11f63e74566802b0dea5ec4f1b4\n"
      ],
      "metadata": {
        "id": "Nz7MTnsUFcyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load training & testing data\n",
        "Load the training and testing data, then split the training data into a validation set using your preferred ratio."
      ],
      "metadata": {
        "id": "AYo4WYfMHws_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE HERE ###\n",
        "# Load X_train, y_train and X_test data\n",
        "# Change the path if needed\n",
        "y_train = np.load('Activity data/y_train.npy')\n",
        "X_train = np.load('Activity data/X_train.npy')\n",
        "X_test = np.load('Activity data/X_test.npy')\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "### END CODE HERE ###\n",
        "\n",
        "### START CODE HERE ###\n",
        "# Define the validation ratio in your preferred way\n",
        "# 1. One-hot encode y_train. (3 classes)\n",
        "# 2. Split the X_train, y_train data into train & validation set.\n",
        "validation_ratio = 0.15\n",
        "None\n",
        "### END CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "kmP1Uo4PlIwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Visualize the training data"
      ],
      "metadata": {
        "id": "h0u30s9HLcIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot a single sample\n",
        "def plot_sample(X, y, sample_index):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(X[sample_index, :, 0], label='Acc X')\n",
        "    plt.plot(X[sample_index, :, 1], label='Acc Y')\n",
        "    plt.plot(X[sample_index, :, 2], label='Acc Z')\n",
        "    plt.plot(X[sample_index, :, 3], label='Gyro X')\n",
        "    plt.plot(X[sample_index, :, 4], label='Gyro Y')\n",
        "    plt.plot(X[sample_index, :, 5], label='Gyro Z')\n",
        "    plt.title(f'Sample {sample_index} - Activity: {np.argmax(y[sample_index])}')\n",
        "    plt.xlabel('Time Step')\n",
        "    plt.ylabel('Sensor Value')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "### START CODE HERE ###\n",
        "# Plot a few samples\n",
        "for i in range(3):              # Change the range to visualize more samples\n",
        "    plot_sample(X_train, y_train, i)\n",
        " ### END CODE HERE ###"
      ],
      "metadata": {
        "id": "XeD88dTg6oSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Contruct and train the model"
      ],
      "metadata": {
        "id": "doO7vRXk2t1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE HERE ###\n",
        "# Construct the model & Set the parameters.\n",
        "input_size = 6  # Number of input features (1 for single sine wave value)\n",
        "rnn_units = 64  # Number of units in the RNN layer\n",
        "dense_units = 32  # Number of units in the Dense layer\n",
        "output_size = 3  # Number of output classes\n",
        "\n",
        "model = Model()\n",
        "model.add(None)\n",
        "...\n",
        "# Train the model\n",
        "model.train(X_train, y_train, X_val, y_val, epochs=50, learning_rate=0.008, batch_size = 32, loss_function='cce')\n",
        "\n",
        "### END CODE HERE ###\n",
        "# Plot the training and validation losses\n",
        "model.plot_losses()"
      ],
      "metadata": {
        "id": "0nRlh-1qSsi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Predict the output and Save it\n",
        "Remember to submit `y_pred.csv` to Kaggle!"
      ],
      "metadata": {
        "id": "_43jkUMbJKsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not modify this part to get the correct output format!!\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "# Save the predicted classes to a CSV file\n",
        "y_pred_df = pd.DataFrame({\n",
        "    'Id': np.arange(len(y_pred_classes)),\n",
        "    'Classes': y_pred_classes\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "y_pred_df.to_csv('y_pred_advanced.csv', index=False)"
      ],
      "metadata": {
        "id": "detzXpDb8mQn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}