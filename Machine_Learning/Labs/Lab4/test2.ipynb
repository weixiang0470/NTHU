{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQAqOS2xwiYG"
      },
      "source": [
        "Mount Google Drive (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dA4DURCWwvWU"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IagZMs0_qjdL"
      },
      "source": [
        "# **Lab 4 : Neural Network**\n",
        "\n",
        "In *lab 4*, you need to finish:\n",
        "\n",
        "1. Basic Part (65%):\n",
        "  Implement a deep neural network from scratch\n",
        "\n",
        "  > * Section 1: Neural network implementation\n",
        "    >> * Part 1: Linear layer\n",
        "    >> * Part 2: Activation function layer\n",
        "    >> * Part 3: Build model\n",
        "\n",
        "  > * Section 2: Loss function\n",
        "    >> * Part 1: Binary cross-entropy loss (BCE)\n",
        "    >> * Part 2: Categorical cross-entropy loss (CCE)\n",
        "    >> * Part 3: Mean square error (MSE)\n",
        "  > * Section 3: Training and prediction\n",
        "    >> * Part 1: Training function & batch function\n",
        "    >> * Part 2: Regression\n",
        "    >> * Part 3: Binary classification\n",
        "\n",
        "\n",
        "2. Advanced Part (30%): Multi class classification\n",
        "3. Report (5%)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGFR00CQvoaH"
      },
      "source": [
        "## **Important  notice**\n",
        "\n",
        "* Please **do not** change the code outside this code bracket in the basic part.\n",
        "  ```\n",
        "  ### START CODE HERE ###\n",
        "  ...\n",
        "  ### END CODE HERE ###\n",
        "  ```\n",
        "\n",
        "* Please **do not** import any other packages in both basic and advanced part\n",
        "\n",
        "* Please **do not** change the random seed **np.random.seed(1)**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BgcgLVV79Bm"
      },
      "source": [
        "## Import Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fmTH9UkeqdYf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib.animation import FuncAnimation\n",
        "\n",
        "outputs = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO31dEFx-C1y"
      },
      "source": [
        "### Common Notation\n",
        "  * $C$: number of classes\n",
        "  * $n$: number of samples\n",
        "  * $f^{[l]}$: the dimension of outputs in layer $l$, but $f^{[0]}$ is the input dimension\n",
        "  * $Z^{[l]} = A^{[l-1]}W^{[l]} + b^{[l]}$\n",
        "      * $Z^{[l]}$: the output of layer $l$ in the shape $(n, f^{[l]})$\n",
        "      * $A^{[l]}$: the activation of $Z^{[l]}$ in the shape $(n, f^{[l]})$, but $A^{[0]}$ is input $X$\n",
        "      * $W^{[l]}$: the weight in layer $l$ in the shape $(f^{[l-1]}, f^{[l]})$\n",
        "      * $b^{[l]}$: the bias in layer $l$ in the shape $(1, f^{[l]})$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wE5z0w8FQLK"
      },
      "source": [
        "# **Basic Part (65%)**\n",
        "In the Basic Part, you will implement a neural network framework capable of handling both regression, binary classification and multi-class classification tasks.\n",
        "\n",
        "**Note:**\n",
        "After implementing each class/function, test it with the provided input variables to verify its correctness. Save the results in the **outputs** dictionary. (The code for testing and saving results is already provided.)\n",
        "## Section 1: Neural network implementation\n",
        "* Part 1: Linear layer\n",
        "> * Step 1: Linear Initialize parameters\n",
        "> * Step 2: Linear forward\n",
        "> * Step 3: Linear backward\n",
        "> * Step 4: Linear update parameters\n",
        "* Part 2: Activation function layer\n",
        "> * Step 1: Activation forward\n",
        "> * Step 2: Activation backward\n",
        "* Part 3: Build model\n",
        "> * Step 1: Model Initialize parameters\n",
        "> * Step 2: Model forward\n",
        "> * Step 3: Model backward\n",
        "> * Step 4: Model update parameters\n",
        "\n",
        "## Section 2: Loss function\n",
        "* Part 1: Binary cross-entropy loss (BCE)\n",
        "* Part 2: Categorical cross-entropy loss (CCE)\n",
        "* Part 3: Mean square error (MSE)\n",
        "\n",
        "## Section 3: Training and prediction\n",
        "* Part 1: Training function & batch function\n",
        "* Part 2: Regression\n",
        "* Part 3: Binary classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w35ZkTwMc00G"
      },
      "source": [
        "## **Section 1: Neural network implementation(30%)**\n",
        "To implement a neural network, you need to complete 3 classes: **Dense**, **Activation**, and **Model**.\n",
        "The process of training a deep neural network is composed of 3 steps: *forward propagation*, *backward propagation*, and *update*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_krGKUNg_Ix"
      },
      "source": [
        "## Part 1: Linear layer (10%)\n",
        "Dense layer (fully-connected layer) performs linear transformation:\n",
        "\n",
        "$Z = AW + b$, where W is weight matrix and b is bias vector.\n",
        "\n",
        "> ### Step 1: Initialize parameters (0%)\n",
        " * You don't need to write this part.\n",
        " * W is randomly initialized using uniform distribution within $[\\text\\{-limit\\}, \\text\\{limit\\}]$, where $\\text\\{limit\\} = \\sqrt{\\frac{6}{\\text\\{fanin\\} + \\text\\{fanout\\}}}$ (fanin: number of input features, fanout: number of output features)\n",
        " * b is initialized to 0\n",
        "\n",
        "> ### Step 2: Linear forward (4%)\n",
        "* Compute Z using matrix multiplication and addition\n",
        "\n",
        "> ### Step 3: Linear backward (4%)\n",
        "* Use backpropagation to compute gradients of loss function with respect to parameters\n",
        "* For layer l: $Z^{[l]} = A^{[l-1]} W^{[l]} + b^{[l]}$ (followed by activation)\n",
        "* Given $dZ^{[l]}$ (gradient of loss with respect to Z), we need to compute three gradients:\n",
        "  * $dW^{[l]}$: gradient of loss with respect to weights\n",
        "  * $db^{[l]}$: gradient of loss with respect to bias\n",
        "  * $dA^{[l-1]}$: gradient of loss with respect to previous layer output\n",
        "\n",
        "> Formulas:\n",
        "$$ dW^{[l]} = \\frac{1}{n} A^{[l-1] T} dZ^{[l]} $$\n",
        "$$ db^{[l]} = \\frac{1}{n} \\sum_{i = 1}^{n} dZ_i^{[l]} $$\n",
        "$$ dA^{[l-1]} = dZ^{[l]} W^{[l] T} $$\n",
        "\n",
        "> ### Step 4: Linear update parameters (2%)\n",
        "* Update parameters using gradient descent:\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "x0KHo8w9yqbY"
      },
      "outputs": [],
      "source": [
        "class Dense():\n",
        "    def __init__(self, n_x, n_y, seed=1):\n",
        "        self.n_x = n_x\n",
        "        self.n_y = n_y\n",
        "        self.seed = seed\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\"\n",
        "        Argument:\n",
        "        self.n_x -- size of the input layer\n",
        "        self.n_y -- size of the output layer\n",
        "        self.parameters -- python dictionary containing your parameters:\n",
        "                           W -- weight matrix of shape (n_x, n_y)\n",
        "                           b -- bias vector of shape (1, n_y)\n",
        "        \"\"\"\n",
        "        sd = np.sqrt(6.0 / (self.n_x + self.n_y))\n",
        "        np.random.seed(self.seed)\n",
        "        W = np.random.uniform(-sd, sd, (self.n_y, self.n_x)).T      # the transpose here is just for the code to be compatible with the old codes\n",
        "        b = np.zeros((1, self.n_y))\n",
        "\n",
        "        assert(W.shape == (self.n_x, self.n_y))\n",
        "        assert(b.shape == (1, self.n_y))\n",
        "\n",
        "        self.parameters = {\"W\": W, \"b\": b}\n",
        "\n",
        "    def forward(self, A):\n",
        "        \"\"\"\n",
        "        Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "        Arguments:\n",
        "        A -- activations from previous layer (or input data) with the shape (n, f^[l-1])\n",
        "        self.cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "\n",
        "        Returns:\n",
        "        Z -- the input of the activation function, also called pre-activation parameter with the shape (n, f^[l])\n",
        "        \"\"\"\n",
        "\n",
        "        # GRADED FUNCTION: linear_forward\n",
        "        ### START CODE HERE ###\n",
        "        W = self.parameters[\"W\"]\n",
        "        b = self.parameters[\"b\"]\n",
        "        print(\"A.shape=\",A.shape,\"W.shape=\",W.shape)\n",
        "        Z = W.T @ A + b\n",
        "        print(\"Z=\",Z,\"A=\",A,\"W=\",W,\"b=\",b)\n",
        "        \n",
        "        print(\"W=\",W.shape,\"b=\",b.shape,\"A=\",A.shape,\"Z=\",Z.shape)\n",
        "        self.cache = (A,W,b)#{\"A\" : A , \"W\" : W, \"b\" : b} Difficult 3\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # assert(Z.shape == (A.shape[0], self.parameters[\"W\"].shape[1]))#Make sure Z is as expected shape\n",
        "\n",
        "        return Z\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        \"\"\"\n",
        "        Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "        Arguments:\n",
        "        dZ -- Gradient of the loss with respect to the linear output (of current layer l), same shape as Z\n",
        "        self.cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "        self.dW -- Gradient of the loss with respect to W (current layer l), same shape as W\n",
        "        self.db -- Gradient of the loss with respect to b (current layer l), same shape as b\n",
        "\n",
        "        Returns:\n",
        "        dA_prev -- Gradient of the loss with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "\n",
        "        \"\"\"\n",
        "        A_prev, W, b = self.cache\n",
        "        m = A_prev.shape[0]\n",
        "\n",
        "        # GRADED FUNCTION: linear_backward\n",
        "        ### START CODE HERE ###\n",
        "        self.dW = 1/m * A_prev.T @ dZ\n",
        "        self.db = 1/m * np.expand_dims(np.sum(dZ,axis=0),axis=0)\n",
        "        dA_prev = dZ @ W.T\n",
        "        #print(\"dZ=\",dZ,\"b=\",b,\"self.db=\",self.db)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        assert (dA_prev.shape == A_prev.shape)\n",
        "        assert (self.dW.shape == self.parameters[\"W\"].shape)\n",
        "        assert (self.db.shape == self.parameters[\"b\"].shape)\n",
        "\n",
        "        return dA_prev\n",
        "\n",
        "    def update(self, learning_rate):\n",
        "        \"\"\"\n",
        "        Update parameters using gradient descent\n",
        "\n",
        "        Arguments:\n",
        "        learning rate -- step size\n",
        "        \"\"\"\n",
        "\n",
        "        # GRADED FUNCTION: linear_update_parameters\n",
        "        ### START CODE HERE ###\n",
        "        self.parameters[\"W\"] += -1 * learning_rate * self.dW\n",
        "        self.parameters[\"b\"] += -1 * learning_rate * self.db\n",
        "        ### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbnVsi6VJMXD"
      },
      "source": [
        "### Test your **Dense class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7HNAWwmg8R7T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W = [[-0.20325375]\n",
            " [ 0.53968259]\n",
            " [-1.22446471]]\n",
            "b = [[0.]]\n",
            "A.shape= (3, 3) W.shape= (3, 1)\n",
            "Z= [[1.5 2.1 2.7]] A= [[0.  1.  2. ]\n",
            " [0.5 1.5 2.5]\n",
            " [1.  2.  3. ]] W= [[0.1]\n",
            " [0.2]\n",
            " [0.3]] b= [[1.1]]\n",
            "W= (3, 1) b= (1, 1) A= (3, 3) Z= (1, 3)\n",
            "Z = [[1.5 2.1 2.7]]\n",
            "A.shape= (3, 3) W.shape= (3, 1)\n",
            "Z= [[-7.037 -6.979 -6.277]] A= [[-0.8  -0.45 -1.11]\n",
            " [-1.65 -2.36  1.14]\n",
            " [-1.02  0.64 -0.86]] W= [[0.3]\n",
            " [0.3]\n",
            " [0.1]] b= [[-6.2]]\n",
            "W= (3, 1) b= (1, 1) A= (3, 3) Z= (1, 3)\n",
            "dA_prev = [[3.5]\n",
            " [6. ]]\n",
            "dW = [[1.625 0.625]]\n",
            "db = [[2.   0.75]]\n",
            "W = [[0.5 2.5]]\n",
            "b = [[-1.  2.]]\n"
          ]
        }
      ],
      "source": [
        "# Initial parameters\n",
        "dense = Dense(3, 1)\n",
        "print(\"W = \" + str(dense.parameters[\"W\"]))\n",
        "print(\"b = \" + str(dense.parameters[\"b\"]))\n",
        "\n",
        "# Linear forward\n",
        "A, W, b = np.array([[0., 1., 2.], [0.5, 1.5, 2.5], [1., 2., 3.]]), np.array([[0.1], [0.2], [0.3]]), np.array([[1.1]])\n",
        "dense = Dense(3, 1)\n",
        "dense.parameters = {\"W\": W, \"b\": b}\n",
        "Z = dense.forward(A)\n",
        "print(\"Z = \" + str(Z))\n",
        "\n",
        "A, W, b = np.array([[-0.80,-0.45,-1.11],[-1.65,-2.36,1.14],[-1.02,0.64,-0.86]]), np.array([[0.3], [0.3], [0.1]]), np.array([[-6.2]])\n",
        "dense = Dense(3, 1)\n",
        "dense.parameters = {\"W\": W, \"b\": b}\n",
        "Z = dense.forward(A)\n",
        "outputs[\"dense_forward\"] = (Z, dense.cache)\n",
        "\n",
        "# Linear backward\n",
        "dZ, linear_cache = np.array([[1.5, 0.5], [2.5, 1.]]), (np.array([[0.5], [1]]), np.array([[2., 1.0]]), np.array([[0.5, 1.]]))\n",
        "dense = Dense(1, 2)\n",
        "dense.cache = linear_cache\n",
        "dA_prev = dense.backward(dZ)\n",
        "print (\"dA_prev = \" + str(dA_prev))\n",
        "print (\"dW = \" + str(dense.dW))\n",
        "print (\"db = \" + str(dense.db))\n",
        "\n",
        "dZ, linear_cache = np.array([[0.52,0.34],[0.76,0.89]]), (np.array([[0.42], [0.68]]), np.array([[0.35, 0.89]]), np.array([[0.12, 0.76]]))\n",
        "dense = Dense(1, 2)\n",
        "dense.cache = linear_cache\n",
        "dA_prev = dense.backward(dZ)\n",
        "outputs[\"dense_backward\"] = (dA_prev, dense.dW, dense.db)\n",
        "\n",
        "# Linear update parameters\n",
        "np.random.seed(1)\n",
        "dense = Dense(1, 2)\n",
        "dense.parameters = {\"W\": np.array([[1.0, 2.0]]), \"b\": np.array([[0.5, 0.5]])}\n",
        "dense.dW = np.array([[0.5, -0.5]])\n",
        "dense.db = np.array([[1.5, -1.5]])\n",
        "dense.update(1.0)\n",
        "print(\"W = \" + str(dense.parameters[\"W\"]))\n",
        "print(\"b = \" + str(dense.parameters[\"b\"]))\n",
        "\n",
        "np.random.seed(1)\n",
        "dense = Dense(3, 4)\n",
        "parameters, grads = {\"W1\": np.random.rand(3, 4), \"b1\": np.random.rand(1,4)}, {\"dW1\": np.random.rand(3, 4), \"db1\": np.random.rand(1,4)}\n",
        "dense.parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
        "dense.dW = grads[\"dW1\"]\n",
        "dense.db = grads[\"db1\"]\n",
        "dense.update(0.1)\n",
        "outputs[\"dense_update_parameters\"] = {\"W\": dense.parameters[\"W\"], \"b\": dense.parameters[\"b\"]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtPtH0j3BFN7"
      },
      "source": [
        "Expected output:\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>W: </td>\n",
        "    <td>[[-0.20325375]  [0.53968259 [-1.22446471]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b: </td>\n",
        "    <td>[[0.]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Z: </td>\n",
        "    <td>[[1.9] [2.2] [2.5]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dA_prev: </td>\n",
        "    <td>[[3.5] [6.0]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW: </td>\n",
        "    <td>[[1.625 0.625]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db: </td>\n",
        "    <td>[[2.0 0.75]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>W: </td>\n",
        "    <td>[[0.5 2.5]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b: </td>\n",
        "    <td>[[-1.  2.]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r5m2W3aXh_A"
      },
      "source": [
        "## Part 2: Activation function layer (10%)\n",
        "\n",
        "Implement forward and backward propagation for activation function layers, including Sigmoid, Softmax, and ReLU.\n",
        "\n",
        "> ### Step 1: Forward Propagation (5%)\n",
        " Implement the following activation functions:\n",
        ">> #### a) Sigmoid\n",
        "- Use the numerically stable version to prevent exponential overflow:\n",
        "  $$\\sigma(Z) = \\begin{cases}\n",
        "    \\frac{1}{1+e^{-Z}},& \\text{if } Z \\geq 0\\\\\n",
        "    \\frac{e^{Z}}{1+e^{Z}}, & \\text{otherwise}\n",
        "  \\end{cases}$$\n",
        "\n",
        ">> #### b) ReLU\n",
        "- Simple implementation:\n",
        "  $$RELU(Z) = \\max(Z, 0)$$\n",
        "\n",
        ">> #### c) Softmax\n",
        "- Implement using the numerically stable version:\n",
        "  $$\\sigma(\\vec{Z})_i = \\frac{e^{Z_i-b}}{\\sum_{j=1}^{C} e^{Z_j-b}}$$\n",
        "  where $b = \\max_{j=1}^{C} Z_j$\n",
        "\n",
        ">> #### d) Linear\n",
        "- You don't need to implement this part\n",
        "\n",
        "> ### Requirements\n",
        "- Each function should return:\n",
        "  1. Activation value \"a\"\n",
        "  2. Cache containing \"z\" for backward propagation\n",
        "\n",
        "> ### Step 2: Backward Propagation (5%)\n",
        "Implement backward functions for:\n",
        "- Sigmoid\n",
        "- ReLU\n",
        "- Softmax\n",
        "- linear\n",
        "\n",
        "> ### General Form\n",
        "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
        "where $g(.)$ is the activation function\n",
        "\n",
        "> ### Specific Implementations\n",
        "\n",
        ">> #### a) Sigmoid Backward\n",
        "$$\\sigma'(Z^{[l]}) = \\sigma(Z^{[l]}) (1 - \\sigma(Z^{[l]}))$$\n",
        "Use numerically stable sigmoid\n",
        "\n",
        ">> #### b) ReLU Backward\n",
        "$$g'(Z^{[l]}) = \\begin{cases}\n",
        "    1,& \\text{if } Z^{[l]} > 0\\\\\n",
        "    0,              & \\text{otherwise}\n",
        "\\end{cases}$$\n",
        "\n",
        ">> #### c) Softmax Backward\n",
        "For the special case of Softmax combined with Categorical Cross-Entropy loss:\n",
        "$$dZ^{[l]} = s - y$$\n",
        "where $s$ is softmax output, $y$ is true label (one-hot vector)\n",
        "\n",
        "Note: This is a simplified form specific to Softmax + CCE loss combination.\n",
        "\n",
        ">> #### d) linear Backward\n",
        "You don't need to implement this part\n",
        "\n",
        "> ### Note\n",
        "For softmax, use the normalized exponential function to prevent overflow, but use the simplified gradient equation for backwards propagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Nnuv8MmebMgg"
      },
      "outputs": [],
      "source": [
        "class Activation():\n",
        "    def __init__(self, activation_function, loss_function):\n",
        "        self.activation_function = activation_function\n",
        "        self.loss_function = loss_function\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, Z):\n",
        "        if self.activation_function == \"sigmoid\":\n",
        "            \"\"\"\n",
        "            Implements the sigmoid activation in numpy\n",
        "\n",
        "            Arguments:\n",
        "            Z -- numpy array of any shape\n",
        "            self.cache -- stores Z as well, useful during backpropagation\n",
        "\n",
        "            Returns:\n",
        "            A -- output of sigmoid(z), same shape as Z\n",
        "            \"\"\"\n",
        "\n",
        "            # GRADED FUNCTION: sigmoid_forward\n",
        "            ### START CODE HERE ###\n",
        "            # Difficult 4\n",
        "            \n",
        "            A = np.where(Z>=0,1 / (1 + np.exp(-Z)),(np.exp(Z) / (1 + np.exp(Z))))\n",
        "            # if Z.T@Z  : A = 1 / (1 + np.exp(-Z))\n",
        "            # else      : A = np.exp(Z) / (1 + np.exp(Z))\n",
        "            #print(\"forward Z:\",Z.shape)\n",
        "            # A = s_Z\n",
        "            self.cache = Z.copy()\n",
        "            ### END CODE HERE ###\n",
        "\n",
        "            return A\n",
        "        elif self.activation_function == \"relu\":\n",
        "            \"\"\"\n",
        "            Implement the RELU function in numpy\n",
        "            Arguments:\n",
        "            Z -- numpy array of any shape\n",
        "            self.cache -- stores Z as well, useful during backpropagation\n",
        "            Returns:\n",
        "            A -- output of relu(z), same shape as Z\n",
        "\n",
        "            \"\"\"\n",
        "\n",
        "            # GRADED FUNCTION: relu_forward\n",
        "            ### START CODE HERE ###\n",
        "            # A = np.array([max(z,np.zeros(1)) for z in Z]) Difficult\n",
        "            A = np.maximum(0, Z)\n",
        "            #A = np.where(Z<0,0,Z)\n",
        "            self.cache = Z.copy()\n",
        "            ### END CODE HERE ###\n",
        "\n",
        "            assert(A.shape == Z.shape)\n",
        "\n",
        "            return A\n",
        "        elif self.activation_function == \"softmax\":\n",
        "            \"\"\"\n",
        "            Implements the softmax activation in numpy\n",
        "\n",
        "            Arguments:\n",
        "            Z -- np.array with shape (n, C)\n",
        "            self.cache -- stores Z as well, useful during backpropagation\n",
        "\n",
        "            Returns:\n",
        "            A -- output of softmax(z), same shape as Z\n",
        "            \"\"\"\n",
        "\n",
        "            # GRADED FUNCTION: softmax_forward\n",
        "            ### START CODE HERE ###\n",
        "            # print(\"Z=\",Z.shape)#(3,4),3 row 4 column #keepdims=True\n",
        "            b = np.max(Z,axis=1,keepdims=True) #b is the maximum value in each row of Z, and keep its dimension as Z\n",
        "            #print(\"np.max(Z)=\",Z,\"b=\",b)\n",
        "            exp_Z = np.exp(Z-b) #Each column of same row will minus coresponding row in b\n",
        "            A =  exp_Z / np.sum(exp_Z,axis=1,keepdims=True) \n",
        "            self.cache = Z.copy()\n",
        "            #np.sum(exp_Z,axis=1,keepdims=True) : Will sum all columns in each row, keep dimension\n",
        "            ### END CODE HERE ###\n",
        "\n",
        "            return A\n",
        "        elif self.activation_function == \"linear\":\n",
        "            \"\"\"\n",
        "            Linear activation (returns Z directly).\n",
        "            \"\"\"\n",
        "            self.cache = Z.copy()\n",
        "            return Z\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation function: {self.activation_function}\")\n",
        "\n",
        "\n",
        "    def backward(self, dA=None, Y=None):\n",
        "        if self.activation_function == \"sigmoid\":\n",
        "            \"\"\"\n",
        "            Implement the backward propagation for a single SIGMOID unit.\n",
        "            Arguments:\n",
        "            dA -- post-activation gradient, of any shape\n",
        "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "            Returns:\n",
        "            dZ -- Gradient of the loss with respect to Z\n",
        "            \"\"\"\n",
        "\n",
        "            # GRADED FUNCTION: sigmoid_backward\n",
        "            ### START CODE HERE ###\n",
        "            Z = self.cache\n",
        "            sig_Z = self.forward(Z)\n",
        "            # print(\"sig_Z=\",sig_Z,\"1-sig_Z\",1-sig_Z)\n",
        "            dZ = dA * sig_Z * (1 - sig_Z)\n",
        "            ### END CODE HERE ###\n",
        "\n",
        "            assert (dZ.shape == Z.shape)\n",
        "\n",
        "            return dZ\n",
        "\n",
        "        elif self.activation_function == \"relu\":\n",
        "            \"\"\"\n",
        "            Implement the backward propagation for a single RELU unit.\n",
        "            Arguments:\n",
        "            dA -- post-activation gradient, of any shape\n",
        "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "            Returns:\n",
        "            dZ -- Gradient of the loss with respect to Z\n",
        "            \"\"\"\n",
        "\n",
        "            # GRADED FUNCTION: relu_backward\n",
        "            ### START CODE HERE ###\n",
        "            Z = self.cache\n",
        "            #print(\"backward Z : \",Z.shape)\n",
        "            # g_Z = np.array([np.ones(1) if z.T@z > 0 else np.zeros(1) for z in Z]) # Diffucult 2, will have problem in Model.forward\n",
        "            g_Z = np.where(Z>0,1,0)\n",
        "            # print(\"relu g_Z\",g_Z,\" dA=\",dA,\" Z=\",Z)\n",
        "            dZ = np.where(dA * g_Z==-0.,np.zeros(1),dA * g_Z)\n",
        "            # dZ = dA\n",
        "            # dZ[Z<=0]=0\n",
        "            ### END CODE HERE ###\n",
        "\n",
        "            assert (dZ.shape == Z.shape)\n",
        "\n",
        "            return dZ\n",
        "\n",
        "        elif self.activation_function == \"softmax\":\n",
        "            \"\"\"\n",
        "            Implement the backward propagation for a [SOFTMAX->CCE LOSS] unit.\n",
        "            Arguments:\n",
        "            Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n",
        "                                      in a Rock-Paper-Scissors, shape: (n, C)\n",
        "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "            Returns:\n",
        "            dZ -- Gradient of the cost with respect to Z\n",
        "            \"\"\"\n",
        "\n",
        "            # GRADED FUNCTION: softmax_backward\n",
        "            ### START CODE HERE ###\n",
        "            Z = self.cache\n",
        "            s = self.forward(Z)\n",
        "            dZ = s - Y\n",
        "            ### END CODE HERE ###\n",
        "\n",
        "            assert (dZ.shape == self.cache.shape)\n",
        "\n",
        "            return dZ\n",
        "\n",
        "        elif self.activation_function == \"linear\":\n",
        "            \"\"\"\n",
        "            Backward propagation for linear activation.\n",
        "            \"\"\"\n",
        "            return dA\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation function: {self.activation_function}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Z = np.array([[-5], [-1], [0], [1], [5]])\n",
        "# for z in Z:\n",
        "#     print(z)\n",
        "# test = np.array([max(z,np.zeros(1)) for z in Z])\n",
        "# print(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDYVMMS2ecCx"
      },
      "source": [
        "### Test your **Activation class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gBuRAoeUC5jV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sigmoid: A = [[0.00669285]\n",
            " [0.26894142]\n",
            " [0.5       ]\n",
            " [0.73105858]\n",
            " [0.99330715]]\n",
            "ReLU: A = [[0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [5]]\n",
            "Softmax: A = \n",
            "[[0.0320586  0.08714432 0.23688282 0.64391426]\n",
            " [0.1748777  0.47536689 0.1748777  0.1748777 ]\n",
            " [0.0320586  0.08714432 0.23688282 0.64391426]]\n",
            "Linear: A = \n",
            "[[ 1  2  3  4]\n",
            " [ 0  1  0  0]\n",
            " [-2 -1  0  1]]\n",
            "Sigmoid: dZ = [[-0.5       ]\n",
            " [-0.26935835]\n",
            " [-0.11969269]\n",
            " [-0.5       ]\n",
            " [-0.73139639]]\n",
            "ReLU: dZ = [[ 0.    1.7 ]\n",
            " [ 0.    0.  ]\n",
            " [-1.14  3.72]]\n",
            "Softmax: dZ = [[-0.96488097  0.70538451  0.25949646]\n",
            " [ 0.09003057 -0.75527153  0.66524096]\n",
            " [ 0.01766842  0.01766842 -0.03533684]]\n",
            "Linear: dZ = \n",
            "[[ 1.2 -0.5  0.8 -0.3]\n",
            " [ 0.4  0.6 -0.9  0.2]\n",
            " [-0.1  0.5 -0.7  0.9]]\n"
          ]
        }
      ],
      "source": [
        "# Activation forward\n",
        "Z = np.array([[-5], [-1], [0], [1], [5]])\n",
        "\n",
        "sigmoid = Activation(\"sigmoid\", 'cross_entropy')\n",
        "A = sigmoid.forward(Z)\n",
        "print(\"Sigmoid: A = \" + str(A))\n",
        "A = sigmoid.forward(np.array([[0.23], [-0.67], [0.45], [0.89], [-0.10]]))\n",
        "outputs[\"sigmoid\"] = (A, sigmoid.cache)\n",
        "\n",
        "relu = Activation(\"relu\", 'cross_entropy')\n",
        "A = relu.forward(Z)\n",
        "print(\"ReLU: A = \" + str(A))\n",
        "A = relu.forward(np.array([[-0.34], [-0.76], [0.21], [-0.98], [0.54]]))\n",
        "outputs[\"relu\"] = (A, relu.cache)\n",
        "\n",
        "Z = np.array([[1, 2, 3, 4],[0, 1, 0, 0],[-2, -1, 0, 1]])\n",
        "softmax = Activation(\"softmax\", 'cross_entropy')\n",
        "A = softmax.forward(Z)\n",
        "print(\"Softmax: A = \\n\" + str(A))\n",
        "A = softmax.forward(np.array([[0.12, -0.56, 0.78, -0.34], [0.45, 0.67, -0.89, 0.23], [-0.14, 0.50, -0.76, 0.98]]))\n",
        "outputs[\"softmax\"] = (A, softmax.cache)\n",
        "\n",
        "linear = Activation(\"linear\", 'mse')\n",
        "A = linear.forward(Z)\n",
        "print(\"Linear: A = \\n\" + str(A))\n",
        "A = linear.forward(np.array([[0.12, -0.56, 0.78, -0.34], [0.45, 0.67, -0.89, 0.23], [-0.14, 0.50, -0.76, 0.98]]))\n",
        "outputs[\"linear\"] = (A, Z)  # For linear activation, cache is just Z\n",
        "\n",
        "# Activation backward\n",
        "dA, cache = np.array([[-2], [-1.37], [-1.14], [-2], [-3.72]]), np.array([[0], [1], [2], [0], [1]])\n",
        "sigmoid = Activation(\"sigmoid\", 'cross_entropy')\n",
        "sigmoid.cache = cache\n",
        "dZ = sigmoid.backward(dA=dA)\n",
        "print(\"Sigmoid: dZ = \"+ str(dZ))\n",
        "dA, cache = np.array([[9.73], [-7.56], [8.34], [-4.12], [6.89]]), np.array([[-5.45], [3.68], [-2.32], [4.51], [-9.27]])\n",
        "sigmoid.cache = cache\n",
        "outputs[\"sigmoid_backward\"] = sigmoid.backward(dA=dA)\n",
        "\n",
        "relu = Activation(\"relu\", 'cross_entropy')\n",
        "dA, cache = np.array([[-2., 1.7 ], [-1.37, 2.], [-1.14, 3.72]]), np.array([[-2, 1], [-1, 0], [2, 1]])\n",
        "relu.cache = cache\n",
        "dZ = relu.backward(dA=dA)\n",
        "print(\"ReLU: dZ = \"+ str(dZ))\n",
        "dA, cache = np.array([[7.24, -3.58], [8.93, 6.45], [-2.11, 9.87]]), np.array([[-4.76, 5.34], [1.98, -7.22], [3.67, -8.56]])\n",
        "relu.cache = cache\n",
        "outputs[\"relu_backward\"] = relu.backward(dA=dA)\n",
        "\n",
        "Y, cache = np.array([[1, 0, 0],[0, 1, 0],[0, 0, 1]]), np.array([[-2, 1, 0],[-1, 0, 1],[-2, -2, 2]])\n",
        "softmax = Activation(\"softmax\", 'cross_entropy')\n",
        "softmax.cache = cache\n",
        "dZ = softmax.backward(Y=Y)\n",
        "print(\"Softmax: dZ = \" + str(dZ))\n",
        "Y, cache = np.array([[0, 1, 0], [0, 1, 0], [1, 0, 0]]), np.array([[-9.45, 7.32, 3.58], [5.61, -8.27, 6.49], [1.23, -4.56, 7.84]])\n",
        "softmax.cache = cache\n",
        "outputs[\"softmax_backward\"] = softmax.backward(Y=Y)\n",
        "\n",
        "linear = Activation(\"linear\", 'mse')\n",
        "dA = np.array([[1.2, -0.5, 0.8, -0.3], [0.4, 0.6, -0.9, 0.2], [-0.1, 0.5, -0.7, 0.9]])\n",
        "dZ = linear.backward(dA=dA)\n",
        "print(\"Linear: dZ = \\n\" + str(dZ))\n",
        "outputs[\"linear_backward\"] = dZ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyyX_xxdEmNp"
      },
      "source": [
        "Expected output:\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Sigmoid: A</td>\n",
        "    <td>[[0.00669285] [0.26894142] [0.5] [0.73105858] [0.99330715]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>ReLU: A</td>\n",
        "    <td>[[0] [0] [0] [1] [5]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Softmax: A</td>\n",
        "    <td>\n",
        "      [[0.0320586 0.08714432 0.23688282 0.64391426]\n",
        "       [0.1748777 0.47536689 0.1748777 0.1748777]\n",
        "       [0.0320586 0.08714432 0.23688282 0.64391426]]\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Linear: A</td>\n",
        "    <td>\n",
        "      [[1 2 3 4]\n",
        "       [0 1 0 0]\n",
        "       [-2 -1 0 1]]\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(with Sigmoid) dZ</td>\n",
        "    <td>[[-0.5] [-0.26935835] [-0.11969269] [-0.5] [-0.73139639]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(with ReLU) dZ</td>\n",
        "    <td>[[0 1.7] [0 0] [-1.14 3.72]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(with Softmax) dZ</td>\n",
        "    <td>\n",
        "      [[-0.96488097 0.70538451 0.25949646]\n",
        "       [0.09003057 -0.75527153 0.66524096]\n",
        "       [0.01766842 0.01766842 -0.03533684]]\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(with Linear) dZ</td>\n",
        "    <td>\n",
        "      [[1.2 -0.5 0.8 -0.3]\n",
        "       [0.4 0.6 -0.9 0.2]\n",
        "       [-0.1 0.5 -0.7 0.9]]\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9vcTYp_yoPu"
      },
      "source": [
        "## Part 3: Model (10%)\n",
        "\n",
        "Use the functions that you had previously written to implement the complete neural network model, including initialization, forward propagation, backward propagation, and parameter updates.\n",
        "\n",
        "> ### Step 1: Model Initialization (0%)\n",
        "Initialize the model by creating linear and activation function layers.\n",
        "\n",
        ">> #### Requirements:\n",
        "- Store linear layers in a list called `linear`\n",
        "- Store activation function layers in a list called `activation`\n",
        "- Use iteration number as seed for each Dense layer initialization\n",
        "\n",
        ">> #### Note:\n",
        "A linear-activation pair counts as a single layer in the neural network.\n",
        "\n",
        "> ### Step 2: Forward Propagation (4%)\n",
        "Implement the model's forward pass by calling each layer's forward function sequentially.\n",
        "\n",
        ">> #### Process:\n",
        "1. For layers 1 to N-1: [LINEAR -> ACTIVATION]\n",
        "2. Final layer: LINEAR -> SIGMOID (binary) or SOFTMAX (multi-class)\n",
        "\n",
        ">> #### Note:\n",
        "For binary classification, use one output node with sigmoid activation. For K-class classification, use K output nodes with softmax activation.\n",
        "\n",
        "> ### Step 3: Backward Propagation (4%)\n",
        "Implement the model's backward pass by calling each layer's backward function in reverse order.\n",
        "\n",
        ">> #### Process:\n",
        "1. Initialize backpropagation:\n",
        "   - Regression:\n",
        "     $$dAL = AL - Y$$\n",
        "   - Binary classification:\n",
        "     $$dAL = - (\\frac{Y}{AL + \\epsilon} - \\frac{1 - Y}{1 - AL + \\epsilon})$$\n",
        "     where $\\epsilon = 10^{-5}$ to prevent division by zero\n",
        "   - Multi-class classification:\n",
        "     Use `softmax_backward` function\n",
        "2. Backpropagate through layers L to 1\n",
        "\n",
        ">> #### Note:\n",
        "Use cached values from the forward pass in each layer's backward function.\n",
        "\n",
        "> ### Step 4: Parameter Update (2%)\n",
        "Update model parameters using gradient descent.\n",
        "\n",
        ">> #### Update Rule:\n",
        "For each layer $l = 1, 2, ..., L$:\n",
        "$$W^{[l]} = W^{[l]} - \\alpha \\cdot dW^{[l]}$$\n",
        "$$b^{[l]} = b^{[l]} - \\alpha \\cdot db^{[l]}$$\n",
        "where $\\alpha$ is the learning rate\n",
        "\n",
        "This revised structure provides a clear, step-by-step breakdown of the model implementation process, mirroring the format used in Part 2. It covers all the essential components while maintaining a concise and logical flow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0JGMzfIDCSVz"
      },
      "outputs": [],
      "source": [
        "class Model():\n",
        "    def __init__(self, units, activation_functions, loss_function):\n",
        "        self.units = units\n",
        "        self.activation_functions = activation_functions\n",
        "        self.loss_function = loss_function\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\"\n",
        "        Initialize layers of the neural network\n",
        "\n",
        "        Arguments:\n",
        "            self.units -- array defining network structure (e.g., [4,4,1]):\n",
        "                - Input layer: 4 nodes\n",
        "                - Hidden layer: 4 nodes\n",
        "                - Output layer: 1 node\n",
        "            self.activation_functions -- activation function for each layer (e.g., [\"relu\",\"sigmoid\"]):\n",
        "                - First layer uses ReLU\n",
        "                - Second layer uses Sigmoid\n",
        "            self.loss_function -- loss function type: \"cross_entropy\" or \"mse\"\n",
        "        \"\"\"\n",
        "        self.linear = []        # Store all Dense layers (weights & biases)\n",
        "        self.activation = []    # Store all activation function layers\n",
        "\n",
        "        for i in range(len(self.units)-1):\n",
        "            dense = Dense(self.units[i], self.units[i+1], i)\n",
        "            self.linear.append(dense)\n",
        "\n",
        "        for i in range(len(self.activation_functions)):\n",
        "            self.activation.append(Activation(self.activation_functions[i], self.loss_function))\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward propagation through the network\n",
        "\n",
        "        Arguments:\n",
        "        X -- input data: shape (n, f)\n",
        "        Returns:\n",
        "        A -- model output:\n",
        "            - For binary classification: probability (0-1)\n",
        "            - For multi-class: probability distribution across classes\n",
        "            - For regression: predicted values\n",
        "        \"\"\"\n",
        "        A = X\n",
        "\n",
        "        # GRADED FUNCTION: model_forward\n",
        "        ### START CODE HERE ###\n",
        "        for i in range(len(self.units) - 1):\n",
        "            Z = self.linear[i].forward(A)\n",
        "            A = self.activation[i].forward(Z)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, AL=None, Y=None):\n",
        "        \"\"\"\n",
        "        Backward propagation to compute gradients\n",
        "\n",
        "        Arguments:\n",
        "            AL -- model output from forward propagation:\n",
        "                - For binary: probability (n,1)\n",
        "                - For multi-class: probabilities (n,C)\n",
        "            Y -- true labels:\n",
        "                - For binary: 0/1 labels (n,1)\n",
        "                - For multi-class: one-hot vectors (n,C)\n",
        "                - For regression: true values (n,1)\n",
        "\n",
        "        Returns:\n",
        "            dA_prev -- gradients for previous layer's activation\n",
        "        \"\"\"\n",
        "\n",
        "        L = len(self.linear)\n",
        "        C = Y.shape[1]\n",
        "\n",
        "        # assertions\n",
        "        warning = 'Warning: only the following 3 combinations are allowed! \\n \\\n",
        "                    1. binary classification: sigmoid + cross_entropy \\n \\\n",
        "                    2. multi-class classification: softmax + cross_entropy \\n \\\n",
        "                    3. regression: linear + mse'\n",
        "        assert self.loss_function in [\"cross_entropy\", \"mse\"], \"you're using undefined loss function!\"\n",
        "        if self.loss_function == \"cross_entropy\":\n",
        "            if Y.shape[1] == 1:  # binary classification\n",
        "                assert self.activation_functions[-1] == 'sigmoid', warning\n",
        "            else:  # multi-class classification\n",
        "                assert self.activation_functions[-1] == 'softmax', warning\n",
        "                assert self.units[-1] == Y.shape[1], f\"you should set last dim to {Y.shape[1]}(the number of classes) in multi-class classification!\"\n",
        "        elif self.loss_function == \"mse\":\n",
        "            assert self.activation_functions[-1] == 'linear', warning\n",
        "            assert self.units[-1] == Y.shape[1], \"output dimension mismatch for regression!\"\n",
        "\n",
        "        # GRADED FUNCTION: model_backward\n",
        "        ### START CODE HERE ###\n",
        "        if self.activation_functions[-1] == \"linear\":\n",
        "            # Initializing the backpropagation\n",
        "            dAL = AL - Y\n",
        "            # Lth layer (LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
        "            dZ = self.activation[-1].backward(dA=dAL)\n",
        "            dA_prev = self.linear[-1].backward(dZ)\n",
        "\n",
        "        elif self.activation_functions[-1] == \"sigmoid\":\n",
        "            # Initializing the backpropagation\n",
        "            e =1*10**-5\n",
        "            dAL = -1*(Y/(AL+e) - (1-Y)/(1-AL+e))\n",
        "\n",
        "            # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
        "            dZ = self.activation[-1].backward(dA=dAL)\n",
        "            # print(\"dA=\",dA.shape,\"dAL=\",dAL.shape,\"dZ=\",dZ.shape)\n",
        "            dA_prev = self.linear[-1].backward(dZ)\n",
        "\n",
        "        elif self.activation_functions[-1] == \"softmax\":\n",
        "            # Initializing the backpropagation\n",
        "            dZ = self.activation[-1].backward(Y=Y)\n",
        "\n",
        "            # Lth layer (LINEAR) gradients. Inputs: \"dZ\". Outputs: \"dA_prev\"\n",
        "            dA_prev = self.linear[-1].backward(dZ)\n",
        "\n",
        "        # Loop from l=L-2 to l=0\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        # Inputs: \"dA_prev\". Outputs: \"dA_prev\"\n",
        "        for i in reversed(range(L-1)):\n",
        "            dZ = self.activation[i].backward(dA_prev)\n",
        "            dA_prev = self.linear[i].backward(dZ)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        return dA_prev\n",
        "\n",
        "    def update(self, learning_rate):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "        learning_rate -- step size\n",
        "        \"\"\"\n",
        "\n",
        "        L = len(self.linear)\n",
        "\n",
        "        # GRADED FUNCTION: model_update_parameters\n",
        "        ### START CODE HERE ###\n",
        "        for i in range(L):\n",
        "            self.linear[i].update(learning_rate)\n",
        "        ### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxQtZMmA1SNc"
      },
      "source": [
        "### Test your **Model class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EGY7_1bjcm-c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial : ----------\n",
            "W1:  [[ 0.5]\n",
            " [-0.1]] \n",
            "b1:  [[ 0.5]\n",
            " [-0.5]]\n",
            "W2:  [ 0.5 -0. ] \n",
            "b2: \n",
            "First train---------\n",
            "A.shape= (2, 1) W.shape= (2, 1)\n",
            "Z= [[2.5]\n",
            " [1.5]] A= [[5]\n",
            " [5]] W= [[ 0.5]\n",
            " [-0.1]] b= [[ 0.5]\n",
            " [-0.5]]\n",
            "W= (2, 1) b= (2, 1) A= (2, 1) Z= (2, 1)\n",
            "A.shape= (2, 1) W.shape= (2,)\n",
            "Z= [[1.25]] A= [[2.5]\n",
            " [1.5]] W= [ 0.5 -0. ] b= [[0]]\n",
            "W= (2,) b= (1, 1) A= (2, 1) Z= (1, 1)\n",
            "Y_hat =  [[0.77729986]]\n",
            "A.shape= (3, 3) W.shape= (3, 1)\n",
            "Z= [[0.24 0.72 2.31]] A= [[ 0.1  1.1  2.9]\n",
            " [-1.2  0.2 -2.5]\n",
            " [ 1.9  2.3  3.7]] W= [[ 0.5]\n",
            " [-0.1]\n",
            " [ 0.3]] b= [[-0.5]]\n",
            "W= (3, 1) b= (1, 1) A= (3, 3) Z= (1, 3)\n",
            "With sigmoid: A = [[0.55971365 0.67260702 0.90970186]]\n",
            "A.shape= (2, 2) W.shape= (2, 1)\n",
            "Z= [[3.8636 1.871 ]] A= [[ 4.35 -5.67]\n",
            " [-7.89  8.12]] W= [[-3.54]\n",
            " [-2.34]] b= [[0.8]]\n",
            "W= (2, 1) b= (1, 1) A= (2, 2) Z= (1, 2)\n",
            "A.shape= (3, 3) W.shape= (3, 1)\n",
            "Z= [[-0.16  0.34  0.4 ]] A= [[ 0.1  1.1  2.9]\n",
            " [-1.2  0.2 -2.5]\n",
            " [ 1.9  2.3  3.7]] W= [[0.1]\n",
            " [0.2]\n",
            " [0.3]] b= [[-0.5]]\n",
            "W= (3, 1) b= (1, 1) A= (3, 3) Z= (1, 3)\n",
            "With ReLU: A = [[0.   0.34 0.4 ]]\n",
            "A.shape= (2, 2) W.shape= (2, 1)\n",
            "Z= [[-46.1261  11.1322]] A= [[ 7.23 -4.56]\n",
            " [ 5.67 -8.9 ]] W= [[-9.12]\n",
            " [ 3.45]] b= [[0.25]]\n",
            "W= (2, 1) b= (1, 1) A= (2, 2) Z= (1, 2)\n",
            "A.shape= (3, 3) W.shape= (3, 3)\n",
            "Z= [[-0.16  1.34  1.  ]\n",
            " [-0.84 -0.34 -0.8 ]\n",
            " [-0.32  0.62  0.18]] A= [[ 0.1  1.1  2.9]\n",
            " [-1.2  0.2 -2.5]\n",
            " [ 1.9  2.3  3.7]] W= [[ 0.1 -0.1 -0.1]\n",
            " [ 0.2 -0.2  0. ]\n",
            " [ 0.3 -0.3  0.1]] b= [[-0.5  0.5  0.1]]\n",
            "W= (3, 3) b= (1, 3) A= (3, 3) Z= (3, 3)\n",
            "With softmax: A = \n",
            "[[0.11531868 0.51682245 0.36785888]\n",
            " [0.27103708 0.4468646  0.28209832]\n",
            " [0.19198639 0.49148158 0.31653204]]\n",
            "A.shape= (3, 3) W.shape= (3, 3)\n",
            "Z= [[-50.6007  37.8885  56.5356]\n",
            " [108.4271 -82.7757 -63.7716]\n",
            " [-10.1738 -11.4442  91.3574]] A= [[-5.12  4.56  7.89]\n",
            " [ 8.34 -6.78  2.45]\n",
            " [ 3.21 -4.67  5.98]] W= [[ 6.23 -7.85  4.56]\n",
            " [-3.21  9.87 -2.34]\n",
            " [ 1.23 -5.67  8.9 ]] b= [[ 4.12 -6.54  7.89]]\n",
            "W= (3, 3) b= (1, 3) A= (3, 3) Z= (3, 3)\n",
            "A.shape= (3, 3) W.shape= (3, 3)\n",
            "Z= [[ 0.84143224 -0.62563275  0.36676625]\n",
            " [ 0.43088605 -0.02684233  0.1144321 ]\n",
            " [ 2.63819704 -0.53389485  0.79302297]] A= [[ 0.  -2.   0.5]\n",
            " [ 1.  -1.   0.5]\n",
            " [ 2.   0.   0.5]] W= [[ 0.09762701  0.08976637 -0.12482558]\n",
            " [ 0.43037873 -0.1526904   0.783546  ]\n",
            " [ 0.20552675  0.29178823  0.92732552]] b= [[0. 0. 0.]]\n",
            "W= (3, 3) b= (1, 3) A= (3, 3) Z= (3, 3)\n",
            "A.shape= (3, 3) W.shape= (3, 1)\n",
            "Z= [[-3.16886174  0.         -0.98381825]] A= [[0.84143224 0.         0.36676625]\n",
            " [0.43088605 0.         0.1144321 ]\n",
            " [2.63819704 0.         0.79302297]] W= [[-0.20325375]\n",
            " [ 0.53968259]\n",
            " [-1.22446471]] b= [[0.]]\n",
            "W= (3, 1) b= (1, 1) A= (3, 3) Z= (1, 3)\n",
            "AL = [[0.04035447 0.5        0.27213482]]\n",
            "Length of layers list = 2\n",
            "A.shape= (3, 3) W.shape= (3, 3)\n",
            "Z= [[ 0.84143224 -0.62563275  0.36676625]\n",
            " [ 0.43088605 -0.02684233  0.1144321 ]\n",
            " [ 2.63819704 -0.53389485  0.79302297]] A= [[ 0.  -2.   0.5]\n",
            " [ 1.  -1.   0.5]\n",
            " [ 2.   0.   0.5]] W= [[ 0.09762701  0.08976637 -0.12482558]\n",
            " [ 0.43037873 -0.1526904   0.783546  ]\n",
            " [ 0.20552675  0.29178823  0.92732552]] b= [[0. 0. 0.]]\n",
            "W= (3, 3) b= (1, 3) A= (3, 3) Z= (3, 3)\n",
            "A.shape= (3, 3) W.shape= (3, 10)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (10,3) (1,10) ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0.5\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.5\u001b[39m], [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.5\u001b[39m]])\n\u001b[1;32m     57\u001b[0m model \u001b[38;5;241m=\u001b[39m Model([\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m10\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross_entropy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m AL \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAL = \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(AL))\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of layers list = \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mlinear)))\n",
            "Cell \u001b[0;32mIn[15], line 49\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# GRADED FUNCTION: model_forward\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m### START CODE HERE ###\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 49\u001b[0m     Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation[i]\u001b[38;5;241m.\u001b[39mforward(Z)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m### END CODE HERE ###\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[10], line 44\u001b[0m, in \u001b[0;36mDense.forward\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m     42\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA.shape=\u001b[39m\u001b[38;5;124m\"\u001b[39m,A\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW.shape=\u001b[39m\u001b[38;5;124m\"\u001b[39m,W\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 44\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ=\u001b[39m\u001b[38;5;124m\"\u001b[39m,Z,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA=\u001b[39m\u001b[38;5;124m\"\u001b[39m,A,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW=\u001b[39m\u001b[38;5;124m\"\u001b[39m,W,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb=\u001b[39m\u001b[38;5;124m\"\u001b[39m,b)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW=\u001b[39m\u001b[38;5;124m\"\u001b[39m,W\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb=\u001b[39m\u001b[38;5;124m\"\u001b[39m,b\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA=\u001b[39m\u001b[38;5;124m\"\u001b[39m,A\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ=\u001b[39m\u001b[38;5;124m\"\u001b[39m,Z\u001b[38;5;241m.\u001b[39mshape)\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,3) (1,10) "
          ]
        }
      ],
      "source": [
        "# Model initialize parameters\n",
        "model = Model([2, 2, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
        "X,W,b = np.array([[5],[5]]),np.array([[0.5],[-0.1]]),np.array([[0.5],[-0.5]])\n",
        "W2,b2 = np.array([0.5,-0.]),np.array([[0]])\n",
        "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
        "model.linear[1].parameters = {\"W\": W2,\"b\":b2}\n",
        "print(\"Initial : ----------\")\n",
        "print(\"W1: \", model.linear[0].parameters[\"W\"], \"\\nb1: \", model.linear[0].parameters[\"b\"])\n",
        "print(\"W2: \", model.linear[1].parameters[\"W\"], \"\\nb2: \")\n",
        "print(\"First train---------\")\n",
        "A = model.forward(X)\n",
        "print(\"Y_hat = \",A)\n",
        "\n",
        "# Model forward\n",
        "A_prev, W, b = np.array([[0.1, 1.1, 2.9],[-1.2, 0.2, -2.5],[1.9, 2.3, 3.7]]), np.array([[0.5], [-0.1], [0.3]]), np.array([[-0.5]])\n",
        "model = Model([3, 1], [\"sigmoid\"], \"cross_entropy\")\n",
        "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
        "A = model.forward(A_prev)\n",
        "print(\"With sigmoid: A = \" + str(A))\n",
        "A_prev, W, b = np.array([[4.35, -5.67], [-7.89, 8.12]]), np.array([[-3.54], [-2.34]]), np.array([[0.8]])\n",
        "model = Model([2, 1], [\"sigmoid\"], \"cross_entropy\")\n",
        "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
        "A = model.forward(A_prev)\n",
        "outputs[\"model_forward_sigmoid\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n",
        "\n",
        "A_prev, W, b = np.array([[0.1, 1.1, 2.9],[-1.2, 0.2, -2.5],[1.9, 2.3, 3.7]]), np.array([[0.1], [0.2], [0.3]]), np.array([[-0.5]])\n",
        "model = Model([3, 1], [\"relu\"], \"cross_entropy\")\n",
        "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
        "A = model.forward(A_prev)\n",
        "print(\"With ReLU: A = \" + str(A))\n",
        "A_prev, W, b = np.array([[7.23, -4.56], [5.67, -8.90]]), np.array([[-9.12], [3.45]]), np.array([[0.25]])\n",
        "model = Model([2, 1], [\"relu\"], \"cross_entropy\")\n",
        "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
        "A = model.forward(A_prev)\n",
        "outputs[\"model_forward_relu\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n",
        "\n",
        "A_prev, W, b = np.array([[0.1, 1.1, 2.9],[-1.2, 0.2, -2.5],[1.9, 2.3, 3.7]]), np.array([[0.1, -0.1, -0.1],[0.2, -0.2, 0.],[0.3, -0.3, 0.1]]), np.array([[-0.5, 0.5, 0.1]])\n",
        "model = Model([3, 3], [\"softmax\"], \"cross_entropy\")\n",
        "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
        "A = model.forward(A_prev)\n",
        "print(\"With softmax: A = \\n\" + str(A))\n",
        "A_prev, W, b = np.array([[-5.12, 4.56, 7.89], [8.34, -6.78, 2.45], [3.21, -4.67, 5.98]]), np.array([[6.23, -7.85, 4.56], [-3.21, 9.87, -2.34], [1.23, -5.67, 8.90]]), np.array([[4.12, -6.54, 7.89]])\n",
        "model = Model([3, 3], [\"softmax\"], \"cross_entropy\")\n",
        "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
        "A = model.forward(A_prev)\n",
        "outputs[\"model_forward_softmax\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n",
        "\n",
        "# binary classification\n",
        "X = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]])\n",
        "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
        "AL = model.forward(X)\n",
        "print(\"AL = \" + str(AL))\n",
        "print(\"Length of layers list = \" + str(len(model.linear)))\n",
        "\n",
        "# multi-class classification\n",
        "X = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]])\n",
        "model = Model([3, 3, 10], [\"relu\", \"softmax\"], \"cross_entropy\")\n",
        "AL = model.forward(X)\n",
        "print(\"AL = \" + str(AL))\n",
        "print(\"Length of layers list = \" + str(len(model.linear)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEmggOxtdMnl"
      },
      "source": [
        "Expected output:\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>W1:</td>\n",
        "    <td>[[ 0.09762701 0.08976637 -0.12482558] [ 0.43037873 -0.1526904 0.783546 ] [ 0.20552675 0.29178823 0.92732552]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b1:</td>\n",
        "    <td>[[0. 0. 0.]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>W2:</td>\n",
        "    <td>[[-0.20325375] [ 0.53968259] [-1.22446471]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b2:</td>\n",
        "    <td>[[0.]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>With Sigmoid:</td>\n",
        "    <td>A = [[0.64565631] [0.20915937] [0.77902611]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>With ReLU:</td>\n",
        "    <td>A = [[0.6 ] [0. ] [1.26]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>With Softmax:</td>\n",
        "    <td>A = [[0.47535001 0.14317267 0.38147732] [0.05272708 0.75380161 0.19347131] [0.68692136 0.05526942 0.25780921]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>AL:</td>\n",
        "    <td>[[0.56058713] [0.55220559] [0.46331713]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Length of layers list:</td>\n",
        "    <td>2</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>AL:</td>\n",
        "    <td>[[0.11637212 0.08186754 0.0924809  0.09675205 0.12819411 0.09664001 0.08448599 0.09067641 0.1294968  0.08303407]\n",
        "         [0.11413265 0.08432761 0.09365443 0.09736489 0.12404237 0.09726785 0.08664355 0.09207969 0.12512634 0.08536063]\n",
        "         [0.09750771 0.07419482 0.08444682 0.10943351 0.09669465 0.11116299 0.08734059 0.12452515 0.13002144 0.08467232]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Length of layers list:</td>\n",
        "    <td>2</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOGsyLXPNGh5"
      },
      "outputs": [],
      "source": [
        "# Model backward\n",
        "AL, Y, linear_activation_cache = np.array([[0.1], [0.2], [0.5], [0.9], [1.0]]), np.array([[0], [0], [1], [1], [1]]), (((np.array([[-2, 2], [-1, 1], [0, 0], [1, -1], [2, -2]]), np.array([[2.0], [1.0]]), np.array([[0.5]])), np.array([[0], [1], [2], [0], [1]])))\n",
        "model = Model([2, 1], [\"sigmoid\"], \"cross_entropy\")\n",
        "model.linear[0].cache = linear_activation_cache[0]\n",
        "model.activation[0].cache = linear_activation_cache[1]\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "print (\"sigmoid:\")\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(model.linear[0].dW))\n",
        "print (\"db = \" + str(model.linear[0].db) + \"\\n\")\n",
        "AL, Y, linear_activation_cache = np.array([[0.35], [0.93], [0.23], [0.72], [0.90]]), np.array([[1], [0], [1], [0], [1]]), (((np.array([[-1, 2], [1, 3], [2, 0], [1, -4], [3, -2]]), np.array([[1.7], [3.2]]), np.array([[0.25]])), np.array([[2], [1], [2], [0], [0]])))\n",
        "model = Model([2, 1], [\"sigmoid\"], \"cross_entropy\")\n",
        "model.linear[0].cache = linear_activation_cache[0]\n",
        "model.activation[0].cache = linear_activation_cache[1]\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "outputs[\"model_backward_sigmoid\"] = (dA_prev, model.linear[0].dW, model.linear[0].db)\n",
        "\n",
        "X, Y = np.array([[-2, 2], [-1, 1], [0, 0], [1, -1], [2, -2]]), np.array([[0], [1], [1], [1], [1]])\n",
        "model = Model([2, 2, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
        "AL = model.forward(X)\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "print (\"relu:\")\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(model.linear[0].dW))\n",
        "print (\"db = \" + str(model.linear[0].db) + \"\\n\")\n",
        "X, Y = np.array([[4.56, -3.21], [-7.85, 6.34], [2.45, -8.90], [5.67, 3.12], [-4.78, 7.89]]), np.array([[1], [1], [0], [1], [0]])\n",
        "model = Model([2, 2, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
        "AL = model.forward(X)\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "outputs[\"model_backward_relu\"] = (dA_prev, model.linear[0].dW, model.linear[0].db)\n",
        "\n",
        "# binary classification\n",
        "X, Y = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]]), np.array([[1], [0], [0]])\n",
        "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
        "AL = model.forward(X)\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "print(\"Binary classification\")\n",
        "print(\"dW1 = \"+ str(model.linear[0].dW))\n",
        "print(\"db1 = \"+ str(model.linear[0].db))\n",
        "print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")\n",
        "\n",
        "# multi-class classification\n",
        "X, Y = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]]), np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
        "model = Model([3, 3, 3], [\"relu\", \"softmax\"], \"cross_entropy\")\n",
        "AL = model.forward(X)\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "print(\"Multi-class classification\")\n",
        "print(\"dW1 = \"+ str(model.linear[0].dW))\n",
        "print(\"db1 = \"+ str(model.linear[0].db))\n",
        "print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")\n",
        "\n",
        "# regression - mse\n",
        "X, Y = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]]), np.array([[2.5], [1.8], [3.2]])\n",
        "model = Model([3, 3, 1], [\"relu\", \"linear\"], \"mse\")\n",
        "AL = model.forward(X)\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "print(\"Regression\")\n",
        "print(\"dW1 = \"+ str(model.linear[0].dW))\n",
        "print(\"db1 = \"+ str(model.linear[0].db))\n",
        "print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6xzEk3-NGh6"
      },
      "source": [
        "Expected output:\n",
        "<table>\n",
        "  <tr>\n",
        "    <th colspan=\"2\">Sigmoid</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dA_prev:</td>\n",
        "    <td>[[ 0.55554938  0.27777469] [ 0.49152369  0.24576184] [-0.41996594 -0.20998297] [-0.55554938 -0.27777469] [-0.39321993 -0.19660997]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW:</td>\n",
        "    <td>[[-0.29446117] [ 0.29446117]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db:</td>\n",
        "    <td>[[-0.03216622]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th colspan=\"2\">ReLU</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dA_prev:</td>\n",
        "    <td>[[-0.01269296 -0.05595562] [ 0.01470136  0.06480946] [ 0.  0. ] [-0.07496777 -0.0327431 ] [-0.07151883 -0.03123674]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW:</td>\n",
        "    <td>[[ 0.0178719  -0.17321413] [-0.0178719   0.17321413]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db:</td>\n",
        "    <td>[[ 0.00335943 -0.11638953]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th colspan=\"2\">Binary Classification</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW1:</td>\n",
        "    <td>[[-0.06277946  0.26602938 -0.37820327] [ 0.  0.05875647  0. ] [-0.01569486  0.05181823 -0.09455082]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db1:</td>\n",
        "    <td>[[-0.03138973  0.10363646 -0.18910163]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dA_prev:</td>\n",
        "    <td>[[-0.02128713  0.03620889 -0.06919444] [ 0.02675119 -0.04550313  0.08695554] [ 0.08406585 -0.52321654 -0.47247201]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th colspan=\"2\">Multi-class Classification</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW1:</td>\n",
        "    <td>[[ 0.16593371  0.33171007 -0.32297709] [ 0.  0.15006987  0. ] [ 0.04148343  0.04541005 -0.08074427]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db1:</td>\n",
        "    <td>[[ 0.08296685  0.0908201  -0.16148854]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dA_prev:</td>\n",
        "    <td>[[-0.04735391  0.08054785 -0.15392528] [ 0.05429414 -0.09235301  0.1764847 ] [ 0.10229066 -0.30227651 -0.34116033]]</td>\n",
        "  </tr>\n",
        "  <th colspan=\"2\">Regression</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW1:</td>\n",
        "    <td>[[ 0.45352627 -1.49031638  2.73218534] [ 0.          1.09795245  0.        ] [ 0.11338157 -0.64706721  0.68304634]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db1:</td>\n",
        "    <td>[[ 0.22676313 -1.29413441  1.36609267]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dA_prev:</td>\n",
        "    <td>[[-0.10931473  0.18594169 -0.35533076] [-0.07704814  0.13105702 -0.25044727] [-0.60730166  3.77977844  3.41319394]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoGA4O8BUCvq"
      },
      "outputs": [],
      "source": [
        "# Model update\n",
        "np.random.seed(1)\n",
        "parameters, grads = {\"W1\": np.random.rand(3, 4).T, \"b1\": np.random.rand(3,1).T, \"W2\": np.random.rand(1,3).T, \"b2\": np.random.rand(1,1).T}, {\"dW1\": np.random.rand(3, 4).T, \"db1\": np.random.rand(3,1).T, \"dW2\": np.random.rand(1,3).T, \"db2\": np.random.rand(1,1).T}\n",
        "model = Model([4, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
        "model.linear[0].parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
        "model.linear[1].parameters = {\"W\": parameters[\"W2\"], \"b\": parameters[\"b2\"]}\n",
        "model.linear[0].dW, model.linear[0].db, model.linear[1].dW, model.linear[1].db = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n",
        "model.update(0.1)\n",
        "print (\"W1 = \"+ str(model.linear[0].parameters[\"W\"]))\n",
        "print (\"b1 = \"+ str(model.linear[0].parameters[\"b\"]))\n",
        "print (\"W2 = \"+ str(model.linear[1].parameters[\"W\"]))\n",
        "print (\"b2 = \"+ str(model.linear[1].parameters[\"b\"]))\n",
        "\n",
        "np.random.seed(1)\n",
        "parameters, grads = {\"W1\": np.random.rand(3, 4).T, \"b1\": np.random.rand(3,1).T, \"W2\": np.random.rand(1,3).T, \"b2\": np.random.rand(1,1).T}, {\"dW1\": np.random.rand(3, 4).T, \"db1\": np.random.rand(3,1).T, \"dW2\": np.random.rand(1,3).T, \"db2\": np.random.rand(1,1).T}\n",
        "model = Model([4, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
        "model.linear[0].parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
        "model.linear[1].parameters = {\"W\": parameters[\"W2\"], \"b\": parameters[\"b2\"]}\n",
        "model.linear[0].dW, model.linear[0].db, model.linear[1].dW, model.linear[1].db = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n",
        "model.update(0.075)\n",
        "outputs[\"model_update_parameters\"] = {\"W1\": model.linear[0].parameters[\"W\"], \"b1\": model.linear[0].parameters[\"b\"], \"W2\": model.linear[1].parameters[\"W\"], \"b2\": model.linear[1].parameters[\"b\"]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t-HfnHZWYIa"
      },
      "source": [
        "Expected output:\n",
        "<table>\n",
        "  <tr>\n",
        "    <th colspan=\"2\">Data Representation</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>W1:</td>\n",
        "    <td>[[ 0.39721186  0.07752363  0.392862 ] [ 0.64025004  0.00469968  0.52183369] [-0.09671178  0.09679955  0.33138026] [ 0.27099015  0.33705631  0.67538482]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b1:</td>\n",
        "    <td>[[ 0.16234149  0.78232848 -0.02592894]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>W2:</td>\n",
        "    <td>[[0.6012798 ] [0.38575324] [0.49003974]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b2:</td>\n",
        "    <td>[[0.05692437]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmSBVaQOSRrk"
      },
      "source": [
        "## **Section 2: Loss function(10%)**\n",
        "In this section, you need to implement the loss function. We use binary cross-entropy loss for binary classification and categorical cross-entropy loss for multi-class classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScdQdj85uC0P"
      },
      "source": [
        "## Part 1: Binary cross-entropy loss (BCE) (5%)\n",
        "Compute the binary cross-entropy loss $L$, using the following formula:  $$-\\frac{1}{n} \\sum\\limits_{i = 1}^{n} (y^{(i)}\\log\\left(a^{[L] (i)}+ϵ\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}+ϵ\\right)), where\\ ϵ=1e-5$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MjBT0eYQaY81"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: compute_BCE_loss\n",
        "\n",
        "def compute_BCE_loss(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the binary cross-entropy loss function using the above formula.\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (n, 1)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (n, 1)\n",
        "\n",
        "    Returns:\n",
        "    loss -- binary cross-entropy loss\n",
        "    \"\"\"\n",
        "\n",
        "    n = Y.shape[0]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    e = 1 * 10**-5\n",
        "    loss = -1/n * np.sum((Y * np.log(AL + e)) + (1-Y)*np.log(1 - AL + e))\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    loss = np.squeeze(loss)      # To make sure your loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(loss.shape == ())\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoV03IzimBEN"
      },
      "source": [
        "### Test your **compute_BCE_loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r07sqnIXaaMv"
      },
      "outputs": [],
      "source": [
        "AL, Y = np.array([[0.9], [0.6], [0.4], [0.1], [0.2], [0.8]]), np.array([[1], [1], [1], [0], [0], [0]])\n",
        "\n",
        "print(\"loss = \" + str(compute_BCE_loss(AL, Y)))\n",
        "outputs[\"compute_BCE_loss\"] = compute_BCE_loss(np.array([[0.12], [0.85], [0.47], [0.33], [0.76], [0.58], [0.09], [0.62]]), np.array([[1], [1], [0], [1], [0], [1], [1], [0]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iRtgOx_IGPo"
      },
      "source": [
        "Expected output:\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>loss: </td>\n",
        "    <td>0.5783820772863568</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aealRyKbcQzG"
      },
      "source": [
        "## Part 2: Categorical cross-entropy loss (CCE) (5%)\n",
        "Compute the categorical cross-entropy loss $L$, using the following formula: $$-\\frac{1}{n} \\sum\\limits_{i = 1}^{n} (y^{(i)}\\log\\left(a^{[L] (i)}+ϵ\\right)),\\ ϵ = 1e-5$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Owx-kTdcfxV5"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: compute_CCE_loss\n",
        "\n",
        "def compute_CCE_loss(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the categorical cross-entropy loss function using the above formula.\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (n, C)\n",
        "    Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n",
        "                                      in a Rock-Paper-Scissors, shape: (n, C)\n",
        "\n",
        "    Returns:\n",
        "    loss -- categorical cross-entropy loss\n",
        "    \"\"\"\n",
        "\n",
        "    n = Y.shape[0]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    e = 1 * 10**-5\n",
        "    loss = -1/n * np.sum(Y*np.log(AL + e))\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    loss = np.squeeze(loss)      # To make sure your loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(loss.shape == ())\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSOsacYQmNAb"
      },
      "source": [
        "### Test your **compute_CCE_loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YbHVAc7hSh3"
      },
      "outputs": [],
      "source": [
        "AL, Y = np.array([[0.8, 0.1, 0.1],[0.6, 0.3, 0.1],[0.4, 0.5, 0.1],[0.1, 0.7, 0.2],[0.2, 0.1, 0.7],[0.4, 0.1, 0.5]]), np.array([[1, 0, 0],[1, 0, 0],[0, 1, 0],[0, 1, 0],[0, 0, 1],[0, 0, 1]])\n",
        "print(\"loss = \" + str(compute_CCE_loss(AL, Y)))\n",
        "outputs[\"compute_CCE_loss\"] = compute_CCE_loss(np.array([[0.7, 0.2, 0.1], [0.2, 0.2, 0.6], [0.3, 0.5, 0.2], [0.8, 0.1, 0.1], [0.7, 0.15, 0.15]]), np.array([[1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9VVIBB5Ic-D"
      },
      "source": [
        "Expected output:\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>loss: </td>\n",
        "    <td>0.4722526144672341</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_XIpJtBpiAX"
      },
      "source": [
        "## Part 3: Mean square error (MSE) (0%)\n",
        "You don't need to write this part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6RHLNGsepygt"
      },
      "outputs": [],
      "source": [
        "# compute_MSE_loss (MSE)\n",
        "def compute_MSE_loss(AL, Y):\n",
        "    m = Y.shape[0]\n",
        "    loss = (1/m) * np.sum(np.square(AL - Y))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8_AYhzfqlbg"
      },
      "source": [
        "## **Section 3: Training and prediction(35%)**\n",
        "In this section, you will apply your implemented neural network to regression and binary classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpFQpiK5eF64"
      },
      "source": [
        "## Helper function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "woCqucFUYXe6"
      },
      "outputs": [],
      "source": [
        "def predict(x, y_true, model):\n",
        "    \"\"\"\n",
        "    This function is used to predict the results of a  L-layer neural network.\n",
        "\n",
        "    Arguments:\n",
        "    x -- data set of examples you would like to label\n",
        "    model -- trained model\n",
        "\n",
        "    Returns:\n",
        "    y_pred -- predictions for the given dataset X\n",
        "    \"\"\"\n",
        "\n",
        "    n = x.shape[0]\n",
        "\n",
        "    # Forward propagation\n",
        "    y_pred = model.forward(x)\n",
        "\n",
        "    # this transform the output and label of binary classification when using sigmoid + cross entropy for evaluation\n",
        "    # eg. y_pred: [[0.8], [0.2], [0.1]] -> [[0.2, 0.8], [0.8, 0.2], [0.9, 0.1]]\n",
        "    # eg. y_true: [[1], [0], [0]] -> [[0, 1], [1, 0], [1, 0]]\n",
        "    if y_pred.shape[-1] == 1:\n",
        "        y_pred = np.array([[1 - y[0], y[0]] for y in y_pred])\n",
        "        if y_true is not None:\n",
        "            y_true = np.array([[1,0] if y == 0 else [0,1] for y in y_true.reshape(-1)])\n",
        "\n",
        "    # make y_pred/y_true become one-hot prediction result\n",
        "    # eg. y_true: [[1, 0, 0], [0, 0, 1], [0, 1, 0]] -> [0, 2, 1]\n",
        "    # eg. y_pred: [[0.2, 0.41, 0.39], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8]] -> [1, 1, 2]\n",
        "    if y_true is not None:\n",
        "        y_true = np.argmax(y_true, axis=1)\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    if y_true is not None:\n",
        "        # compute accuracy\n",
        "        correct = 0\n",
        "        for yt, yp in zip(y_true, y_pred):\n",
        "            if yt == yp:\n",
        "                correct += 1\n",
        "        print(f\"Accuracy: {correct/n * 100:.2f}%\")\n",
        "\n",
        "        f1_scores = f1_score(y_true, y_pred, average=None)\n",
        "        print(f'f1 score for each class: {f1_scores}')\n",
        "        print(f'f1_macro score: {np.mean(np.array(f1_scores)):.2f}')\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "def save_prediction_data(predicted_y):\n",
        "    # Create DataFrame with ID, x, and y columns\n",
        "    df = pd.DataFrame({\n",
        "        'ID': range(len(predicted_y)),  # Add ID column starting from 0\n",
        "        'y': predicted_y\n",
        "    })\n",
        "\n",
        "    # Ensure ID is the first column\n",
        "    df = df[['ID', 'y']]\n",
        "\n",
        "    # Save to CSV file\n",
        "    df.to_csv('Lab4_basic_regression.csv', index=False)\n",
        "    print(\"Prediction data saved as 'Lab4_basic_regression.csv'\")\n",
        "\n",
        "def animate_training(history, X_train, Y_train):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlim(0, 11)\n",
        "    ax.set_ylim(-5, 5)\n",
        "    line, = ax.plot([], [], 'b-', lw=1, label='Predicted')\n",
        "\n",
        "    ground_truth_x = X_train.flatten()\n",
        "    ground_truth_y = Y_train.flatten()\n",
        "    ax.plot(ground_truth_x, ground_truth_y, 'r-', lw=1, label='Ground Truth')\n",
        "\n",
        "    # show current epoch on the animation / 100 epoch\n",
        "    epoch_text = ax.text(0.05, 0.95, '', transform=ax.transAxes, fontsize=12, verticalalignment='top')\n",
        "\n",
        "    def init():\n",
        "        line.set_data([], [])\n",
        "        epoch_text.set_text('')\n",
        "        return line, epoch_text\n",
        "\n",
        "    def update(frame):\n",
        "        epoch = (frame + 1) * 100\n",
        "        _, predicted_y = history[frame]\n",
        "        predicted_x = X_train.flatten()\n",
        "        line.set_data(predicted_x, predicted_y.flatten())\n",
        "\n",
        "        epoch_text.set_text(f'Epoch: {epoch}')\n",
        "\n",
        "        return line, epoch_text\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=len(history), init_func=init, blit=True, interval=50)\n",
        "\n",
        "    # save as gif\n",
        "    ani.save('Lab4_basic_regression.gif', writer='pillow')\n",
        "    plt.close(fig)\n",
        "    print(f\"Animation saved as 'Lab4_basic_regression.gif'\")\n",
        "\n",
        "\n",
        "def save_final_result(model, X_train, Y_train):\n",
        "    AL = model.forward(X_train)\n",
        "\n",
        "    predicted_x = X_train.flatten()\n",
        "    predicted_y = AL.flatten()\n",
        "\n",
        "    plt.plot(predicted_x, predicted_y, 'b-', label=\"Predicted\", lw=1)\n",
        "\n",
        "    ground_truth_x = X_train.flatten()\n",
        "    ground_truth_y = Y_train.flatten()\n",
        "\n",
        "    save_prediction_data(predicted_y)\n",
        "\n",
        "    plt.plot(ground_truth_x, ground_truth_y, 'r-', label='Ground Truth', lw=1)\n",
        "\n",
        "    plt.legend()\n",
        "\n",
        "    plt.ylim(-5, 5)\n",
        "    plt.xlim(0, 11)\n",
        "    plt.savefig(\"Lab4_basic_regression.jpg\")\n",
        "    plt.show()\n",
        "    print(\"Prediction saved as 'Lab4_basic_regression.jpg'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgVRVmOYG9FK"
      },
      "source": [
        "## Part1: Training function & batch function (5%)\n",
        "The functions defined in this part will be utilized in the subsequent training parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fjOBHI0bGVE7"
      },
      "outputs": [],
      "source": [
        "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "\n",
        "    Arguments:\n",
        "    X -- input data, of shape (n, f^{0})\n",
        "    Y -- true \"label\" vector, of shape (n, C)\n",
        "    mini_batch_size -- size of the mini-batches, integer\n",
        "\n",
        "    Returns:\n",
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[0]                  # number of training examples\n",
        "    mini_batches = []\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # Step 1: Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[permutation]\n",
        "    shuffled_Y = Y[permutation]\n",
        "\n",
        "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
        "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
        "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        mini_batch_X = shuffled_X[k*mini_batch_size:(k+1)*mini_batch_size]\n",
        "        mini_batch_Y = shuffled_Y[k*mini_batch_size:(k+1)*mini_batch_size]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "\n",
        "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
        "    if m % mini_batch_size != 0:\n",
        "        mini_batch_X = shuffled_X[num_complete_minibatches*mini_batch_size:m]\n",
        "        mini_batch_Y = shuffled_Y[num_complete_minibatches*mini_batch_size:m]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return mini_batches\n",
        "\n",
        "def train_model(model, X_train, Y_train,x_val,y_val, learning_rate, num_iterations, batch_size=None, print_loss=True, print_freq=1000, decrease_freq=100, decrease_proportion=0.99,is_val=False):\n",
        "    \"\"\"\n",
        "    Trains the model using mini-batch gradient descent\n",
        "\n",
        "    Arguments:\n",
        "    model -- the model to be trained\n",
        "    X_train -- training set, of shape (num_px * num_px * 3, m_train)\n",
        "    Y_train -- training labels, of shape (1, m_train)\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    batch_size -- size of a mini batch\n",
        "    print_loss -- if True, print the loss every print_freq iterations\n",
        "    print_freq -- print frequency\n",
        "    decrease_freq -- learning rate decrease frequency\n",
        "    decrease_proportion -- learning rate decrease proportion\n",
        "\n",
        "    Returns:\n",
        "    model -- the trained model\n",
        "    losses -- list of losses computed during the optimization\n",
        "    history -- list of (X_train, Y_pred) tuples for visualization\n",
        "    \"\"\"\n",
        "\n",
        "    history = []\n",
        "    losses = []\n",
        "    for i in range(num_iterations):\n",
        "        ### START CODE HERE ###\n",
        "        if i==0 : \n",
        "            first = True\n",
        "            global best_f1\n",
        "            global best_f1_iter\n",
        "        # Define mini batches\n",
        "        if batch_size is not None:\n",
        "            mini_batches = random_mini_batches(X_train,Y_train,batch_size)\n",
        "        else:\n",
        "            # if batch_size is None, batch is not used, mini_batch = whole dataset\n",
        "            mini_batches = [(X_train,Y_train)]\n",
        "\n",
        "        epoch_loss = 0\n",
        "        for batch in mini_batches:\n",
        "            X_batch, Y_batch = batch\n",
        "            \n",
        "            # Forward pass\n",
        "            AL = model.forward(X_batch)\n",
        "\n",
        "            # Compute loss\n",
        "            if model.loss_function == 'cross_entropy':\n",
        "                if model.activation_functions[-1] == \"sigmoid\": # Binary classification\n",
        "                    loss = compute_BCE_loss(AL,Y_batch)\n",
        "                elif model.activation_functions[-1] == \"softmax\": # Multi-class classification\n",
        "                    loss = compute_CCE_loss(AL,Y_batch)\n",
        "            elif model.loss_function == 'mse': # Regression\n",
        "                loss = compute_MSE_loss(AL,Y_batch)\n",
        "            epoch_loss += loss\n",
        "\n",
        "            # Backward pass\n",
        "            model.backward(AL,Y_batch)\n",
        "\n",
        "            # Update parameters\n",
        "            model.update(learning_rate)\n",
        "\n",
        "        epoch_loss /= len(mini_batches)\n",
        "        losses.append(epoch_loss)\n",
        "        if is_val : \n",
        "            # predict(x_val, y_val, model)\n",
        "            # if y_val is not None and first:\n",
        "            #     y_val = np.argmax(y_val, axis=1)\n",
        "            #     first = False\n",
        "            f1_scores = MY_predict(x_val, y_val, model)\n",
        "            if best_f1 < np.mean(np.array(f1_scores)):\n",
        "                best_f1 = np.mean(np.array(f1_scores))\n",
        "                best_f1_iter = i\n",
        "                print(f\"Current best training iteration is : {best_f1_iter}, f1 scores = {best_f1}\")\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Print loss\n",
        "        if print_loss and i % print_freq == 0:\n",
        "            print(f\"Loss after iteration {i}: {epoch_loss}\")\n",
        "\n",
        "        # Store history\n",
        "        if i % 100 == 0:\n",
        "            history.append((X_train, model.forward(X_train)))\n",
        "\n",
        "        # Decrease learning rate\n",
        "        if i % decrease_freq == 0 and i > 0:\n",
        "            learning_rate *= decrease_proportion\n",
        "\n",
        "    return model, losses, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2D8lubAw8pX"
      },
      "source": [
        "## Part 2: Regression (10%)\n",
        "In this part, Your task is to train a neural network model to approximate the following mathematical function:\n",
        "\n",
        "$$y = sin(2 * sin(2 * sin(2 * sin(x))))$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-ksy7v-Hrrt"
      },
      "source": [
        "> ### Step 1: Data generation\n",
        "Generate the mathematical function :  $$y = sin(2 * sin(2 * sin(2 * sin(x))))$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0yRO6y_FyLPM"
      },
      "outputs": [],
      "source": [
        "def generate_data(num_points=1000):\n",
        "\n",
        "    x = np.linspace(0.01, 11, num_points)\n",
        "    y = np.sin(2 * np.sin(2 * np.sin(2 * np.sin(x))))\n",
        "\n",
        "    return x.reshape(-1, 1), y.reshape(-1, 1)\n",
        "x_train, y_train = generate_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC_BxLJJHxHD"
      },
      "source": [
        "> ### Step 2: Train model\n",
        "Implement and train your model using the generated data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alsJ4F6eHZ2a"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ###\n",
        "x_train, y_train = generate_data()\n",
        "loss_function = \"mse\"\n",
        "layers_dims = [1,20,40,20,40,1]\n",
        "activation_fn = [\"relu\",\"sigmoid\",\"relu\",\"relu\",\"linear\"]\n",
        "learning_rate = 0.3\n",
        "num_iterations = 40000\n",
        "print_loss = True\n",
        "print_freq = 1000\n",
        "decrease_freq = 1000\n",
        "decrease_proportion = 0.93\n",
        "# You don't necessarily need to use mini_batch in this part\n",
        "batch_size = None\n",
        "\n",
        "best_f1,best_f1_iter=0,0\n",
        "x_val,y_val = None,None\n",
        "model = Model(layers_dims, activation_fn, loss_function)\n",
        "model, losses, history = train_model(model, x_train, y_train,x_val,y_val, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion)\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Plot the loss\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.title(f'Training Loss (Initial LR: {learning_rate})')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQQtLyFgL4WD"
      },
      "source": [
        "> ### Step 3: Save prediction\n",
        "Save your model's predictions to:\n",
        "> * *Lab4_basic_regression.csv*\n",
        "> * *Lab4_basic_regression.jpg*\n",
        "> * *Lab4_basic_regression.gif*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uwle3uqL9Em"
      },
      "outputs": [],
      "source": [
        "save_final_result(model, x_train, y_train)\n",
        "animate_training(history, x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## My Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### My predict function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 478,
      "metadata": {},
      "outputs": [],
      "source": [
        "def MY_predict(x, y_true, model):\n",
        "    \"\"\"\n",
        "    This function is used to predict the results of a  L-layer neural network.\n",
        "\n",
        "    Arguments:\n",
        "    x -- data set of examples you would like to label\n",
        "    model -- trained model\n",
        "\n",
        "    Returns:\n",
        "    y_pred -- predictions for the given dataset X\n",
        "    \"\"\"\n",
        "\n",
        "    n = x.shape[0]\n",
        "\n",
        "    # Forward propagation\n",
        "    y_pred = model.forward(x)\n",
        "\n",
        "    # this transform the output and label of binary classification when using sigmoid + cross entropy for evaluation\n",
        "    # eg. y_pred: [[0.8], [0.2], [0.1]] -> [[0.2, 0.8], [0.8, 0.2], [0.9, 0.1]]\n",
        "    # eg. y_true: [[1], [0], [0]] -> [[0, 1], [1, 0], [1, 0]]\n",
        "    if y_pred.shape[-1] == 1:\n",
        "        y_pred = np.array([[1 - y[0], y[0]] for y in y_pred])\n",
        "        if y_true is not None:\n",
        "            y_true = np.array([[1,0] if y == 0 else [0,1] for y in y_true.reshape(-1)])\n",
        "\n",
        "    # make y_pred/y_true become one-hot prediction result\n",
        "    # eg. y_true: [[1, 0, 0], [0, 0, 1], [0, 1, 0]] -> [0, 2, 1]\n",
        "    # eg. y_pred: [[0.2, 0.41, 0.39], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8]] -> [1, 1, 2]\n",
        "    if y_true is not None:\n",
        "        y_true = np.argmax(y_true, axis=1)\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    if y_true is not None:\n",
        "        # compute accuracy\n",
        "        correct = 0\n",
        "        for yt, yp in zip(y_true, y_pred):\n",
        "            if yt == yp:\n",
        "                correct += 1\n",
        "        f1_scores = f1_score(y_true, y_pred, average=None)\n",
        "\n",
        "    return f1_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Increase image contrast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 479,
      "metadata": {},
      "outputs": [],
      "source": [
        "def increase_contrast(X, factor):\n",
        "    \"\"\"\n",
        "    Increase the contrast of the images in the array X.\n",
        "\n",
        "    Parameters:\n",
        "    X -- input images as a 2D NumPy array of shape (num_data, 784)\n",
        "    factor -- contrast increase factor (1 means no change)\n",
        "\n",
        "    Returns:\n",
        "    contrasted_images -- the contrasted images as a 2D NumPy array\n",
        "    \"\"\"\n",
        "    X = np.array(X)\n",
        "    mean = np.mean(X, axis=1, keepdims=True) # Calculate mean brightness\n",
        "    contrasted_images = (X - mean) * factor + mean # Change image's contrast\n",
        "    contrasted_images = np.clip(contrasted_images, 0, 255).astype(np.uint8) # Change back to 0~255\n",
        "\n",
        "    return contrasted_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalize and split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 480,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize(X):\n",
        "    \"\"\"\n",
        "    Normalize data to [0,1] range\n",
        "\n",
        "    Parameters:\n",
        "    X -- input images as a 2D Numpy array of shape (num_data,784)\n",
        "\n",
        "    Returns:\n",
        "    Normalized images array\n",
        "    \"\"\"\n",
        "    return ((X - X.min(axis=0))/(X.max(axis=0) - X.min(axis=0)))\n",
        "\n",
        "def split_X_Y(X,Y,split_ratio):\n",
        "    \"\"\"\n",
        "    Split X,Y into training and validation data through split_ratio\n",
        "\n",
        "    Parameters:\n",
        "    X -- features of a image array (num_data,784)\n",
        "    Y -- ground true of data (num_data,1)\n",
        "    split_ratio -- the ratio to split to training data and validation data\n",
        "\n",
        "    Returns:\n",
        "    splited training data and validation data (X_train,Y_train,X_validation,Y_validation)\n",
        "    \"\"\"\n",
        "    n_train = int(len(X) * split_ratio)\n",
        "    indices = np.random.choice(len(X), size=n_train, replace=False)\n",
        "    X_t = X[indices]\n",
        "    Y_t = Y[indices]\n",
        "    val_indices = np.setdiff1d(np.arange(len(X)), indices)\n",
        "    X_val = X[val_indices]\n",
        "    Y_val = Y[val_indices]\n",
        "    return X_t,Y_t,X_val,Y_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX-kDur7xKOW"
      },
      "source": [
        "## Part 3: Binary classification (10%)\n",
        "\n",
        "You will train a model to perform binary classification. Your task is to predict whether an optical coherence tomography (OCT) image shows Choroidal Neovascularization (CNV) or is normal.\n",
        "\n",
        "- Data: OCT scan image of retina\n",
        "- Classes:\n",
        "  - CNV: label = 1\n",
        "  - Normal: label = 0\n",
        "\n",
        "- Data Description:\n",
        "  - Input: Grayscale images (28x28 pixels)\n",
        "  - Training set size: 20000 images\n",
        "  - Testing set size: 5000 images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAvUwG1uSLg_"
      },
      "source": [
        "> ### Step 1: Read data & split data\n",
        "Load *basic_data.npz* and prepare it for training by splitting into training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp8M93z7v_lO"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "data = np.load('basic_data.npz')\n",
        "X_train = data[\"x_train\"]\n",
        "Y_train = data[\"y_train\"]\n",
        "X_test = data[\"x_test\"]\n",
        "\n",
        "# Display sample images with labels\n",
        "class_names_binary = {0: 'Normal', 1: 'CNV'}\n",
        "plt.figure(figsize=(5, 5))\n",
        "for i in range(9):\n",
        "    plt.subplot(330 + 1 + i)\n",
        "    plt.imshow(X_train[i].reshape(28, 28), cmap='gray', vmin=0, vmax=255)\n",
        "    plt.title(f'Label: {int(Y_train[i])} ({class_names_binary[int(Y_train[i])]})', fontsize=8)\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Data preprocessing\n",
        "### START CODE HERE ###\n",
        "# print(X_train.shape)\n",
        "# print(Y_train.shape)\n",
        "# Normalize X data to [0,1] range\n",
        "# print(X_train[0,:10])\n",
        "X_train = increase_contrast(X_train, factor=2)\n",
        "X_test = increase_contrast(X_test, factor=2)\n",
        "X_train = normalize(X_train)\n",
        "# print(X_train[0,:10])\n",
        "X_test = normalize(X_test)\n",
        "# Reshape Y_train to 2D array\n",
        "Y_train = Y_train.reshape(-1,1)\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Plot data distribution\n",
        "Y_train_1 = np.sum(Y_train == 1)\n",
        "Y_train_0 = np.sum(Y_train == 0)\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.bar([0, 1], [Y_train_0, Y_train_1])\n",
        "plt.title('Data distribution')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "print('Train: x=%s, y=%s' % (X_train.shape, Y_train.shape))\n",
        "print('Test: x=%s' % (X_test.shape, ))\n",
        "\n",
        "# Train-validation split\n",
        "### START CODE HERE ###\n",
        "# Choose the ratio for splitting\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "# print(indices)\n",
        "x_train,y_train,x_val,y_val = split_X_Y(X_train,Y_train,split_ratio)\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(\"\\nAfter splitting:\")\n",
        "print(\"x_train:\", x_train.shape, \"| y_train:\", y_train.shape)\n",
        "print(\"x_val:\", x_val.shape, \"| y_val:\", y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r01QzzHxeMbR"
      },
      "source": [
        "> ### Step 2: Training and Evaluation\n",
        "Train your model on the prepared OCT image data and evaluate its performance in distinguishing between CNV and normal retinal conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI7JY5ESjhZ2"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ###\n",
        "loss_function = \"cross_entropy\"\n",
        "layers_dims = [784,256,128,64,1]\n",
        "activation_fn = [\"relu\",\"relu\",\"relu\",\"sigmoid\"]\n",
        "learning_rate = 0.1\n",
        "num_iterations = 52 # Best training iteration is : 52\n",
        "decrease_freq = 50\n",
        "decrease_proportion = 0.7\n",
        "batch_size = 32\n",
        "print_loss = True\n",
        "print_freq = 50\n",
        "best_f1 = 0\n",
        "best_f1_iter = 0\n",
        "# You might need to use mini_batch to reduce training time in this part\n",
        "\n",
        "model = Model(layers_dims, activation_fn, loss_function)\n",
        "model, losses, history = train_model(model, x_train, y_train,x_val,y_val, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion,is_val=True)\n",
        "print(f\"Best training iteration is : {best_f1_iter}, f1 scores = {best_f1}\")\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Plot the loss\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.title(f'Training Loss (Initial LR: {learning_rate})')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8q0a20XcPtk"
      },
      "outputs": [],
      "source": [
        "print('training------')\n",
        "pred_train = predict(x_train, y_train, model)\n",
        "print('validation------')\n",
        "pred_val = predict(x_val, y_val, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqtnepD-6I20"
      },
      "source": [
        "> ### Step 3: Save prediction\n",
        "Save your model's predictions to: *Lab4_basic.csv*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mERo3g41zsyX"
      },
      "outputs": [],
      "source": [
        "pred_test = predict(X_test, None, model)\n",
        "df = pd.DataFrame({\n",
        "    'ID': range(len(pred_test)),\n",
        "    'Label': pred_test.flatten()\n",
        "})\n",
        "\n",
        "df.to_csv('Lab4_basic.csv', index=False)\n",
        "print(\"Prediction data saved as 'Lab4_basic.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMCpPFMVdj36"
      },
      "source": [
        "# **Advanced Part (30%)**\n",
        "\n",
        "You will train a model to perform multi-class classification on medical imaging data. Your task is to classify optical coherence tomography (OCT) images of retinal conditions into four different categories.\n",
        "\n",
        "- Data: OCT scan images of retina\n",
        "- Classes:\n",
        "  - CNV (Choroidal Neovascularization): label = 0\n",
        "  - DME (Diabetic Macular Edema): label = 1\n",
        "  - Drusen: label = 2\n",
        "  - Normal: label = 3\n",
        "\n",
        "- Data Description:\n",
        "  - Input: Grayscale images (28x28 pixels)\n",
        "  - Training set size: 37754 images\n",
        "  - Testing set size: 6997 images\n",
        "\n",
        "**Notes:** You can implement other functions to improve your rankings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_GQ3uO128OC"
      },
      "source": [
        "## Step 1: Read data & split data\n",
        "\n",
        "Load *advanced_data.npz* and prepare it for training by splitting into training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVSfqnXqXGdC"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "data = np.load('advanced_data.npz')\n",
        "X_train = data[\"x_train\"]\n",
        "Y_train = data[\"y_train\"]\n",
        "X_test = data[\"x_test\"]\n",
        "\n",
        "print(f'Initial shapes:')\n",
        "print(f'Train: X={X_train.shape}, Y={Y_train.shape}')\n",
        "print(f'Test: X={X_test.shape}')\n",
        "\n",
        "# Display sample images with labels\n",
        "class_names = {0: 'CNV', 1: 'DME', 2: 'Drusen', 3: 'Normal'}\n",
        "plt.figure(figsize=(5, 5))\n",
        "for i in range(9):\n",
        "    plt.subplot(330 + 1 + i)\n",
        "    plt.imshow(X_train[i].reshape(28, 28), cmap='gray', vmin=0, vmax=255)\n",
        "    plt.title(f'Label: {int(Y_train[i])} ({class_names[int(Y_train[i])]})', fontsize=8)\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Data preprocessing\n",
        "### START CODE HERE ###\n",
        "# Convert labels to one-hot encoding\n",
        "def one_hot_encode(Y,nums):\n",
        "    one_hot = np.zeros((Y.size,nums))\n",
        "    # print(one_hot)\n",
        "    one_hot[np.arange(Y.size), Y] = 1\n",
        "    # print(Y)\n",
        "    # print(one_hot)\n",
        "    return one_hot\n",
        "\n",
        "num_classes = 4  # OCT has 4 classes\n",
        "Y_train = one_hot_encode(Y_train,num_classes)\n",
        "\n",
        "# Normalize X data to [0,1] range\n",
        "X_train = increase_contrast(X_train, factor=2)\n",
        "X_test = increase_contrast(X_test, factor=2)\n",
        "X_train = normalize(X_train)\n",
        "X_test = normalize(X_test)\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(\"\\nAfter preprocessing:\")\n",
        "print(\"shape of X_train:\", X_train.shape)\n",
        "print(\"shape of Y_train:\", Y_train.shape)\n",
        "print(\"shape of X_test:\", X_test.shape)\n",
        "\n",
        "# Plot class distribution before splitting\n",
        "orig_labels = np.argmax(Y_train, axis=1)\n",
        "unique, counts = np.unique(orig_labels, return_counts=True)\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.bar(unique, counts)\n",
        "plt.title('Class Distribution')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Train-validation split\n",
        "### START CODE HERE ###\n",
        "# Choose the ratio for splitting\n",
        "split_ratio = 0.95\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train,y_train,x_val,y_val = split_X_Y(X_train,Y_train,split_ratio)\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(\"\\nAfter splitting:\")\n",
        "print(\"x_train:\", x_train.shape, \"| y_train:\", y_train.shape)\n",
        "print(\"x_val:\", x_val.shape, \"| y_val:\", y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngmUDGN13ADi"
      },
      "source": [
        "## Step 2: Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 486,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 0.65229 , contrast 0.76921\n",
        "# layers_dims = [784,512,256,128,4]\n",
        "# activation_fn = [\"relu\",\"relu\",\"relu\",\"softmax\"]\n",
        "# learning_rate = 0.1\n",
        "# num_iterations = 200\n",
        "# decrease_freq = 25\n",
        "# decrease_proportion = 0.9\n",
        "# batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIi1A-1dFY0u"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ###\n",
        "loss_function = \"cross_entropy\"\n",
        "layers_dims = [784,512,256,128,4]\n",
        "activation_fn = [\"relu\",\"relu\",\"relu\",\"softmax\"]\n",
        "learning_rate = 0.1\n",
        "num_iterations = 200\n",
        "decrease_freq = 25\n",
        "decrease_proportion = 0.9\n",
        "batch_size = 32\n",
        "print_loss = True\n",
        "print_freq = 50\n",
        "best_f1 = 0\n",
        "best_f1_iter = 0\n",
        "\n",
        "model = Model(layers_dims, activation_fn, loss_function)\n",
        "model, losses, history= train_model(model, x_train, y_train,x_val,y_val, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion,is_val=True)\n",
        "print(f\"Best training iteration is : {best_f1_iter}, f1 scores = {best_f1}\")\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Plot the loss\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.title(f'Training Loss (Initial LR: {learning_rate})')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehjcfSU2XD3-"
      },
      "outputs": [],
      "source": [
        "print('training------')\n",
        "pred_train = predict(x_train, y_train, model)\n",
        "print('validation------')\n",
        "pred_val = predict(x_val, y_val, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXGnS3HQeNUc"
      },
      "source": [
        "## Step 3: Save prediction\n",
        "Save your model's predictions to: *Lab4_advanced.csv*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHFDuq2BQ2qI"
      },
      "outputs": [],
      "source": [
        "pred_test = predict(X_test, None, model)\n",
        "df = pd.DataFrame({\n",
        "    'ID': range(len(pred_test)),\n",
        "    'Label': pred_test.flatten()\n",
        "})\n",
        "df.to_csv('Lab4_advanced.csv', index=False)\n",
        "print(\"Prediction data saved as 'Lab4_advanced.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J91ff4Vk1oB_"
      },
      "source": [
        "# Save outputs\n",
        "Save the outputs of your testing codes to: *Lab4_output.npy*\n",
        "\n",
        "We will test your *Lab4_output.npy* to verify the correctness of your neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 495,
      "metadata": {
        "id": "CpxmIFiW1tg9"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "assert list(outputs.keys()) == [\n",
        "    'dense_forward',\n",
        "    'dense_backward',\n",
        "    'dense_update_parameters',\n",
        "    'sigmoid',\n",
        "    'relu',\n",
        "    'softmax',\n",
        "    'linear',\n",
        "    'sigmoid_backward',\n",
        "    'relu_backward',\n",
        "    'softmax_backward',\n",
        "    'linear_backward',\n",
        "    'model_forward_sigmoid',\n",
        "    'model_forward_relu',\n",
        "    'model_forward_softmax',\n",
        "    'model_backward_sigmoid',\n",
        "    'model_backward_relu',\n",
        "    'model_update_parameters',\n",
        "    'compute_BCE_loss',\n",
        "    'compute_CCE_loss'\n",
        "], \"You're missing something, please restart the kernel and run the code from beginning to the end. If the same error occurs, maybe you deleted some outputs, check the template to find the missing parts!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDqCzhsp1yTb"
      },
      "outputs": [],
      "source": [
        "np.save(\"Lab4_output.npy\", outputs)\n",
        "\n",
        "# sanity check for saved outputs\n",
        "submit = np.load(\"Lab4_output.npy\", allow_pickle=True).item()\n",
        "for key, value in submit.items():\n",
        "    print(f\"{key}: {type(value)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
