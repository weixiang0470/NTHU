{"cells":[{"cell_type":"markdown","metadata":{"id":"IagZMs0_qjdL"},"source":["# 1. Introduction\n","\n","Welcome to the Lab5. In this lab, you will build a convolutional neural network step by step. In this notebook, you will implement all the functions required to build a convolutional neural network.\n","\n","After finishing this lab, you will have a deeper understanding of the process of training a convolutional neural network, which mainly consists of two parts: convolution layer and pooling layer."]},{"cell_type":"markdown","metadata":{"id":"yGFR00CQvoaH"},"source":["# 2. Import Packages\n","\n","1. To build a convolutional neural network, we start by importing the Dense layer, Activation layer, and Loss function that you implemented in Lab4. Ensure the following three files are located in the same directory as this notebook, and follow the instructions to complete the setup:\n","    - Dense.py : Copy the **Dense class** you had implemented in Lab4 to it.\n","    - Activation.py : Copy the **Activation class** you had implemented in Lab4 to it.\n","    - Loss.py : Copy **compute_BCE_loss** function you had implemented in Lab4 to it.\n","\n","2. Helper function\n","    - Predict.py : This file contains a helper function for model prediction and evaluation. **No modifications are required** for this file.\n","\n","⚠️ **WARNING** ⚠️:\n","*   Please do not import any other packages in this lab.\n","*   np.random.seed(seed) is used to keep all the random function calls consistent. It will help us grade your work. Please don't change the seed.\n","\n","❗ **Important** ❗: Please do not change the code outside this code bracket.\n","```\n","### START CODE HERE ### (≈ n lines)\n","...\n","### END CODE HERE ###\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Uf8X57wOQo1w"},"source":["Mount Google Drive (optional)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YcLLrIEc-4h6"},"outputs":[],"source":["### START CODE HERE ###\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","### END CODE HERE ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fmTH9UkeqdYf"},"outputs":[],"source":["import os\n","import math\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","\n","###### import your HW4 code######\n","from Dense import Dense\n","from Activation import Activation\n","from Loss import compute_BCE_loss\n","from Predict import predict\n","##################################\n","\n","output = {}\n","seed = 1\n","np.random.seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"xme1U0TCQo1x"},"source":["# 3. Basic part"]},{"cell_type":"markdown","metadata":{"id":"VMe4BNRPRQvF"},"source":["## 3.1 Convolution layer\n","\n","Convolution layer enables us to capture the important features of input images.\n","\n","In this section, we will focus on convolution layer. The implemented function will be gradually incorporated into this class, so you should use self.function() whenever you need to call it."]},{"cell_type":"markdown","metadata":{"id":"vzSk8alpQo1x"},"source":["### 3.1.1 Initialize the Convolution layer\n","\n","First, we initialize the Convolution layer and set up the weights and biases of the convolutional filters using Glorot uniform initialization.\n","\n","- It will take following parameters to initialize the convolution layer:\n","\n","    *   filter_size: Defines the dimensions of the filter, which will be of shape (filter_size x filter_size).\n","    \n","    *   input_channel: Specifies the size of the input channel.\n","    \n","    *   output_channel: Specifies the size of the output channel.\n","    \n","    *   pad: The amount of padding applied around each image along the vertical and horizontal dimensions.\n","    \n","    *   stride: The number of steps the filter moves during each operation.\n","    \n","**Note: No implementation is required for this section.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rClniXjDQo1x"},"outputs":[],"source":["class Conv():\n","    def __init__(self, filter_size=2, input_channel=3, output_channel=8, pad=1, stride=1, seed=1):\n","\n","        self.filter_size= filter_size\n","        self.input_channel=input_channel\n","        self.output_channel=output_channel\n","        self.seed = seed\n","        self.pad = pad\n","        self.stride = stride\n","\n","        self.parameters = {'W':None, 'b': None}\n","        self.initialize_parameters()\n","\n","    def initialize_parameters(self):\n","        \"\"\"\n","        self.parameters -- python dictionary containing your parameters:\n","                           W -- weight matrix of shape (filter_size, filter_size, input channel size, output channel size)\n","                           b -- bias vector of shape (1, 1, 1, output channel size)\n","        \"\"\"\n","        np.random.seed(seed)\n","        sd = np.sqrt(6.0 / (self.input_channel + self.output_channel))\n","        W = np.random.uniform(-sd, sd, (self.filter_size,self.filter_size,self.input_channel,self.output_channel))\n","        b = np.zeros((1, 1, 1, self.output_channel))\n","\n","        assert(W.shape == (self.filter_size,self.filter_size,self.input_channel,self.output_channel))\n","        assert(b.shape == (1,1,1,self.output_channel))\n","\n","        self.parameters['W'] = W\n","        self.parameters['b'] = b"]},{"cell_type":"markdown","metadata":{"id":"WI-1JUPjQo1y"},"source":["### 3.1.2 Zero-Padding\n","\n","Implement the zero_pad() function to pad the input X with the given parameter on vertical and horizontal dimensions with zero.\n","\n","- It allows you to use a convolution layer without necessarily shrinking the height and width of the volumes. This is important for building deeper networks since otherwise the height/width would shrink as you go to deeper layers.\n","\n","- It helps us keep more of the information at the border of an image. Without padding, very few values at the next layer would be affected by pixels as the edges of an image.\n","\n","**Note:** This function is **not included** as a method in the Conv class, so you can call zero_pad() directly whenever you need to use it.\n","\n","**Hint:** You can use function [np.pad](https://numpy.org/doc/2.0/reference/generated/numpy.pad.html) to add the specified amount of zero-padding around image on both the vertical and horizontal dimensions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ADlgENHVRQvG"},"outputs":[],"source":["def zero_pad(X, pad):\n","    \"\"\"\n","    Pad all images in the dataset X with zeros. The padding should be applied to both the height and width of each image.\n","\n","    Argument:\n","    X -- python numpy array of shape (m, n_H, n_W, n_C), where m represent the number of examples.\n","    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n","\n","    Returns:\n","    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n","    \"\"\"\n","\n","    ### START CODE HERE ###\n","    X_pad = ...\n","    ### END CODE HERE ###\n","\n","    return X_pad"]},{"cell_type":"markdown","metadata":{"id":"-ukYzbawQo1y"},"source":["#### **Test and Evaluate** the **zero_pad** function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hRpzM6qxQo1y"},"outputs":[],"source":["np.random.seed(seed)\n","x = np.random.randn(4, 3, 3, 2)\n","x_pad = zero_pad(x, 2)\n","print (\"x.shape =\", x.shape)\n","print (\"x_pad.shape =\", x_pad.shape)\n","print (\"x[0,2,:,0] =\", x[0,2,:,0])\n","print (\"x_pad[0,2,:,0] =\", x_pad[0,2,:,0])\n","\n","fig, axarr = plt.subplots(1, 2)\n","axarr[0].set_title('x')\n","axarr[0].imshow(x[0,:,:,0])\n","axarr[1].set_title('x_pad')\n","axarr[1].imshow(x_pad[0,:,:,0])\n","\n","np.random.seed(seed)\n","x = np.random.randn(4, 2, 2, 2)\n","x_pad = zero_pad(x, 1)\n","output[\"zero_padding\"] = x_pad[0,1,:,0]"]},{"cell_type":"markdown","metadata":{"id":"iG3MiZBlQo1y"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>x.shape: </td>\n","    <td>(4, 3, 3, 2)</td>\n","  </tr>\n","  <tr>\n","    <td>x_pad.shape: </td>\n","    <td>(4, 7, 7, 2)</td>\n","  </tr>\n","  <tr>\n","    <td>x[0,2,:,0]: </td>\n","    <td>[-0.3224172   1.13376944 -0.17242821]</td>\n","  </tr>\n","  <tr>\n","    <td>x_pad[0,2,:,0]: </td>\n","    <td>[ 0. 0. 1.62434536 -0.52817175 0.86540763 0. 0.]\n","</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"CDvCjY4LQo1y"},"source":["### 3.1.3 Convolution Single Step\n","\n","In this section, you will implement the **conv_single_step** function which will apply a filter to a single region f * f of the input data.\n","\n","<span style=\"font-size: 25px;\">Key Concepts</span>\n","\n","* Convolution is performed using a sliding window of size f * f, where f is the filter size.\n","  \n","* This function applies a convolution filter of dimensions (f, f, n_c_prev) on an input slice of shape (f, f, n_c_prev), resulting in a single scalar value.\n","\n","* In section 3.1.4, we would slide the filter along (H,W) coordinate to get a 2D feature map for the filter\n","\n","<span style=\"font-size: 25px;\">Steps in the conv_single_step implementation</span>\n","\n","1. **Step 1:** Do element-wise product to a_slice_prev and W to get shape (f, f, n_c_prev).\n","   \n","2. **Step 2:** Sum all values to get a single scalar, reducing the (f, f, n_C_prev) matrix into a single scalar.\n","   \n","3. **Step 3:** Add the bias b to the scalar result. Cast b to a float() so that Z results in a scalar value."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vQzL2D3yQo1y"},"outputs":[],"source":["def conv_single_step(self, a_slice_prev, W, b):\n","        \"\"\"\n","        Arguments:\n","        a_slice_prev -- slice of previous activation layer output with shape (filter_size, filter_size, n_C_prev)\n","        W -- Weight parameters contained in a window - matrix of shape (filter_size, filter_size, n_C_prev)\n","        b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n","\n","        Returns:\n","        Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n","        \"\"\"\n","\n","        ### START CODE HERE ### (≈ 3 lines)\n","        # Step 1: Element-wise product to a_slice_prev and W\n","        ...\n","        # Step 2: Sum all values to get a single scalar\n","        ...\n","        # Step 3: Add the bias\n","        ...\n","        ### END CODE HERE ###\n","\n","        return Z\n","\n","Conv.conv_single_step = conv_single_step"]},{"cell_type":"markdown","metadata":{"id":"1YKNLaiJQo1z"},"source":["#### **Test and Evaluate** the **conv_single_step** function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02WmPxJKJbJa"},"outputs":[],"source":["np.random.seed(seed)\n","a_slice_prev = np.random.randn(4, 4, 3)\n","W = np.random.randn(4, 4, 3)\n","b = np.random.randn(1, 1, 1)\n","\n","conv = Conv(filter_size=2, input_channel=3, output_channel=8, pad=2, stride=2)\n","Z = conv.conv_single_step(a_slice_prev, W, b)\n","print(\"Z =\", Z)\n","\n","np.random.seed(seed)\n","a_slice_prev = np.random.randn(3, 3, 3)\n","W = np.random.randn(3, 3, 3)\n","b = np.random.randn(1, 1, 1)\n","conv = Conv()\n","Z = conv.conv_single_step(a_slice_prev, W, b)\n","output[\"conv_single_step\"] = Z"]},{"cell_type":"markdown","metadata":{"id":"SVHY5VIFVLiC"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>Z: </td>\n","    <td>-6.999089450680221</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"QfVBJ8ScQo1z"},"source":["### 3.1.4 Forward Pass\n","\n","In this section, you will take multiple filters and convolve them through the input. Each filter produces a 2D matrix output, and stacking these matrices creates a 3D output volume.\n","\n","The **conv_single_step** function is essential for this process. During the **Forward pass**, we call **conv_single_step** repeatedly on different slices of the input along (H,W) coordinate to build the entire 2D output matrix for each filter. This involves the following steps:\n","\n","**Step 1: Calculate the Output Dimension**:\n","   \n","- The final output of the convolution operation is a 3D volume with dimensions (n_H, n_W, n_C):\n","\n","  - Height: n_H $= \\left\\lfloor \\frac{H_{prev} - filter\\_size + 2 \\times \\text{pad}}{\\text{stride}} \\right\\rfloor + 1$\n","\n","  - Width: n_W $= \\left\\lfloor \\frac{W_{prev} - filter\\_size + 2 \\times \\text{pad}}{\\text{stride}} \\right\\rfloor + 1$\n","\n","  - Depth: n_C, which is the number of filters\n","\n","**Step 2: Padding**:\n","\n","- Pad the input based on the padding value to ensure that we correctly calculate the output volumn for each position.\n","\n","**Step 3: Loop Through Training Examples**:\n","\n","The input data has dimensions (m, n_H_prev, n_W_prev, n_C_prev), where m represents the number of input data.\n","\n","For each training example, follow these steps to compute the output volumn:\n","\n","- **Step 3-1: Extracting slices**:\n","\n","  - For each position (h, w, c) in the output matrix, we define a slice of the input within the sliding window where each slice has dimensions (f, f, n_C_prev).\n","\n","  - In each operation, we slide the window with a defined stride along the (H, W) coordinates to extract different slice.\n","\n","- **Step 3-2: Applying Filters**:\n","\n","  - For each slice of shape (f, f, n_C_prev), we apply a filter also of shape (f, f, n_C_prev) using **conv_single_step**.\n","\n","  - This element-wise multiplication and summation over the entire slice and filter result in a single scalar output, reducing (f, f, n_C_prev) to (1, 1, 1).\n","\n","  - By iterating over all positions (h, w) across the height and width of the input, **conv_single_step** computes the result at each position, building up a 2D matrix of size (n_H, n_W) for each filter.\n","\n","  - By iterating over all filters (which correspond to the output channels), we build up a 3D output volume of shape (n_H, n_W, n_C), where n_C is the number of filters used in the layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H0SJ42G2Qo1z"},"outputs":[],"source":["def forward(self, A_prev):\n","    \"\"\"\n","    Implements the forward propagation for a convolution layer\n","\n","    Arguments:\n","    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n","\n","    Returns:\n","    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n","    \"\"\"\n","\n","    ### START CODE HERE ###\n","    # Retrieve dimensions from A_prev's shape\n","    (m, n_H_prev, n_W_prev, n_C_prev) = ...\n","\n","    # Retrieve dimensions from W's shape\n","    (f, f, n_C_prev, n_C) = ...\n","\n","\n","    # Step 1: Output Dimension Calculation\n","    pad = ...\n","    stride = ...\n","    n_H = ...\n","    n_W = ...\n","\n","    # Initialize the output volume Z with zeros\n","    Z = ...\n","\n","    # Step 2: Padding\n","    A_prev_pad = ...\n","\n","    # Step 3: Loop Through Training Examples\n","    for i in range(m):                                 # loop over the batch of training examples\n","        for h in range(n_H):                           # loop over vertical axis of the output volume\n","            for w in range(n_W):                       # loop over horizontal axis of the output volume\n","                for c in range(n_C):                   # loop over channels (= #filter) of the output volume\n","\n","\n","                    # Step 3-1: Extracting slices\n","                    vert_start = ...\n","                    vert_end = ...\n","                    horiz_start = ...\n","                    horiz_end = ...\n","                    a_slice_prev = ...\n","\n","                    # Step 3-2: Applying Filters\n","                    ...\n","\n","    ### END CODE HERE ###\n","\n","    # Making sure your output shape is correct\n","    assert(Z.shape == (m, n_H, n_W, n_C))\n","\n","    # Save information in \"cache\" for the backward pass\n","    self.cache = A_prev\n","\n","    return Z\n","\n","Conv.forward = forward"]},{"cell_type":"markdown","metadata":{"id":"fDu808HWQo1z"},"source":["#### **Test and Evaluate** the **forward** function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Ags0LKKRQvH"},"outputs":[],"source":["np.random.seed(seed)\n","A_prev = np.random.randn(10,4,4,3)\n","conv=Conv(filter_size=2, input_channel=3, output_channel=8, pad=2, stride=2)\n","Z = conv.forward(A_prev)\n","\n","print(\"Z's shape =\", Z.shape)\n","print(\"Z's mean =\", np.mean(Z))\n","print(\"Z[3,2,1] =\", Z[3,2,1])\n","\n","\n","np.random.seed(seed)\n","A_prev = np.random.randn(10,3,3,2)\n","conv=Conv(filter_size=3, input_channel=2, output_channel=16, pad=1, stride=1)\n","Z = conv.forward(A_prev)\n","\n","output[\"conv_forward_1\"] = Z.shape\n","output[\"conv_forward_2\"] = np.mean(Z)\n","output[\"conv_forward_3\"] = Z[3,2,1]"]},{"cell_type":"markdown","metadata":{"id":"5qiBeJbhVTlU"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>Z's shape: </td>\n","    <td>(10, 4, 4, 8)</td>\n","  </tr>\n","  <tr>\n","    <td>Z's mean: </td>\n","    <td>0.0031904169881830785</td>\n","  </tr>\n","  <tr>\n","    <td>Z[3,2,1]: </td>\n","    <td>[ 1.32947002  2.12083471  0.37853495 -3.53602735  1.38816885 -1.01503137\n"," -1.01667531  0.86993377]</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"45MLJaL6Qo1z"},"source":["### 3.1.5 Backward Pass\n","\n","In this section, we calculate the gradients of the cost function with respect to the input, weights, and biases of the convolutional layer. This allows us to update these parameters during training. The backward pass involves the following gradients:\n","\n","- **dA_prev**: Gradient of the cost with respect to the input of the conv layer, with dimensions (m, n_H_prev, n_W_prev, n_C_prev).\n","  \n","- **dW**: Gradient of the cost with respect to the weights of the conv layer, with dimensions (f, f, n_C_prev, n_C).\n","  \n","- **db**: Gradient of the cost with respect to the biases of the conv layer, with dimensions (1, 1, 1, n_C).\n","\n","<span style=\"font-size: 25px;\">Backward Pass Steps</span>\n","\n","Given the gradient of the cost with respect to the output of the convolutional layer, dZ, which has dimensions (m, n_H, n_W, n_C), the backward pass proceeds as follows:\n","\n","**Step 1: Initialize Gradients**:\n","\n","- Create dA_prev, dW, and db with the correct shapes\n","\n","**Step 2: Padding**:\n","\n","- Pad A_prev and dA_prev based on the padding value to ensure that we correctly calculate the gradients for each position\n","\n","**Step 3: Loop Through Training Examples**:\n","\n","For each training example, follow these steps to compute and update the gradients:\n","\n","- **Step 3-1: Extracting slices**:\n","\n","  - This step is the same as what you have implemented in **Forward pass**. The slice is used to calculate the gradient of filter's weight.\n","\n","- **Step 3-2: Update the Gradients**:\n","\n","    $$\n","  dA^{l-1} = \\frac{\\partial L}{\\partial Z^l} \\times \\frac{\\partial Z^l}{\\partial A^{l-1}} = \\frac{\\partial L}{\\partial A^{l-1}}\n","  $$\n","\n","  $$\n","  dW^l = \\frac{1}{m} \\times \\frac{\\partial L}{\\partial Z^l} \\times \\frac{\\partial Z^l}{\\partial W^l} = \\frac{\\partial L}{\\partial W^l}\n","  $$\n","\n","  $$\n","  db^l = \\frac{1}{m} \\times \\frac{\\partial L}{\\partial Z^l} \\times \\frac{\\partial Z^l}{\\partial b^l} = \\frac{\\partial L}{\\partial b^l}\n","  $$\n","\n","  **Hint:**\n","  - $ \\frac{\\partial L}{\\partial Z^l} $ corresponds to $ dZ $\n","  - m is the batch size\n","  - $ Z_{h,w,c}^l = A_{f \\times f}^{l-1} * W_{c}^l + b_{c}^l $ , where f is the kernel size\n","  \n","  Iterating over the height, width, and channels of the output to compute the full set of gradients.\n","\n","**Step 4: Remove Padding**:\n","- After calculating the gradients for all slices, remove the padding from dA_prev_pad to obtain dA_prev with shape (m, n_H_prev, n_W_prev, n_C_prev).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TPrUOk-cQo1z"},"outputs":[],"source":["def backward(self, dZ):\n","    \"\"\"\n","    Implement the backward propagation for a convolution layer\n","\n","    Arguments:\n","    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n","\n","    Returns:\n","    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n","                numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n","    \"\"\"\n","\n","\n","    A_prev = self.cache\n","\n","    ### START CODE HERE ###\n","\n","    # Retrieve dimensions from A_prev's shape\n","    (m, n_H_prev, n_W_prev, n_C_prev) = ...\n","\n","    # Retrieve dimensions from W's shape\n","    (f, f, n_C_prev, n_C) = ...\n","\n","    # Retrieve dimensions from dZ's shape\n","    (m, n_H, n_W, n_C) = ...\n","\n","    # Step 1: Initialize Gradients\n","    dA_prev = ...\n","    dW = ...\n","    db = ...\n","\n","    # Step 2: Padding\n","    A_prev_pad = ...\n","    dA_prev_pad = ...\n","\n","    # Step 3: Loop Through Training Examples\n","    for i in range(m):                         # loop over the batch of training examples\n","        for h in range(n_H):                   # loop over vertical axis of the output volume\n","            for w in range(n_W):               # loop over horizontal axis of the output volume\n","                for c in range(n_C):           # loop over the channels of the output volume\n","\n","                    # Step 3-1: Extracting slices\n","                    vert_start = ...\n","                    vert_end = ...\n","                    horiz_start = ...\n","                    horiz_end = ...\n","                    a_slice = ...\n","\n","                    # Step 3-2: Update the Gradients\n","                    ...\n","\n","        # Step 4: Remove Padding\n","        dA_prev[i, :, :, :] = ...\n","\n","    ### END CODE HERE ###\n","\n","    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n","\n","    self.dW = dW\n","    self.db = db\n","\n","    return dA_prev\n","\n","Conv.backward = backward"]},{"cell_type":"markdown","metadata":{"id":"Otcu5-e9Qo10"},"source":["#### **Test and Evaluate** the **backward** function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r4bSxlOLQo10"},"outputs":[],"source":["np.random.seed(seed)\n","dZ = np.random.randn(10,4,4,8)\n","conv = Conv(filter_size=2, input_channel=3, output_channel=8, pad=2, stride=2)\n","conv.cache = np.random.randn(10,4,4,3)\n","dA_prev = conv.backward(dZ)\n","\n","print(\"dA_prev's shape =\", dA_prev.shape)\n","print(\"dA_prev's mean =\", np.mean(dA_prev))\n","print(\"dA_prev[3,2,1] =\", dA_prev[3,2,1])\n","print(\"dW[0,0,0,:] =\", conv.dW[0,0,0,:])\n","print(\"db[0,0,0,:] =\", conv.db[0,0,0,:])\n","\n","\n","np.random.seed(seed)\n","dZ = np.random.randn(10,3,3,16)\n","conv = Conv(filter_size=3, input_channel=2, output_channel=16, pad=1, stride=1)\n","conv.cache = np.random.randn(10,3,3,2)\n","dA_prev = conv.backward(dZ)\n","\n","output[\"conv_backward_1\"] = dA_prev.shape\n","output[\"conv_backward_2\"] = np.mean(dA_prev)\n","output[\"conv_backward_3\"] = dA_prev[3,2,1]"]},{"cell_type":"markdown","metadata":{"id":"yug_W_MYQo10"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>dA_prev's shape: </td>\n","    <td>(10, 4, 4, 3)</td>\n","  </tr>\n","  <tr>\n","    <td>dA_prev's mean: </td>\n","    <td>0.00655067636329092</td>\n","  </tr>\n","  <tr>\n","    <td>Z[3,2,1]: </td>\n","    <td>[-0.33079703 -1.64413855  0.34342549]</td>\n","  </tr>\n","  <tr>\n","    <td>dW[0,0,0,:]: </td>\n","    <td>[ 0.628421    0.27688478 -0.15725264  0.60455304  0.16843747  1.00921225, 0.55441834  0.1224805 ]</td>\n","  </tr>\n","  <tr>\n","    <td>db[0,0,0,:]: </td>\n","    <td>[ 2.83837596  1.10463954  0.17494534  0.94603986 -1.05306856 -0.45070565 0.15993941  1.32191626]</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"bbGLz2F_ReRr"},"source":["### 3.1.6 Update parameters\n","In this section, you will update the parameters of the convolution layer, using gradient descent:\n","\n","$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$\n","$$b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jF5XbAziQo10"},"outputs":[],"source":["def update(self, learning_rate):\n","    \"\"\"\n","    Update parameters using gradient descent\n","\n","    Arguments:\n","    learning rate -- step size\n","    \"\"\"\n","\n","    ### START CODE HERE ###\n","    self.parameters[\"W\"] = ...\n","    self.parameters[\"b\"] = ...\n","    ### END CODE HERE ###\n","\n","Conv.update = update"]},{"cell_type":"markdown","metadata":{"id":"th-w_C4vQo10"},"source":["#### **Test and Evaluate** the **update** function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QOw8N6q7RgGU"},"outputs":[],"source":["conv=Conv(filter_size=2, input_channel=3, output_channel=5, pad=2, stride=2)\n","np.random.seed(seed)\n","conv.dW = np.random.randn(2, 2, 3, 5)\n","conv.db = np.random.randn(1, 1, 1, 5)\n","conv.update(1.0)\n","print(\"W[0,0,0:2] = \", conv.parameters[\"W\"][0,0,0:2])\n","print(\"b = \", conv.parameters[\"b\"])\n","\n","conv=Conv(filter_size=3, input_channel=3, output_channel=5, pad=2, stride=2)\n","np.random.seed(seed)\n","conv.dW = np.random.randn(3, 3, 3, 5)\n","conv.db = np.random.randn(1, 1, 1, 5)\n","conv.update(1.0)\n","output[\"conv_update_1\"] = conv.parameters[\"W\"][0,0,0:2]\n","output[\"conv_update_2\"] = conv.parameters[\"b\"]\n"]},{"cell_type":"markdown","metadata":{"id":"3WBrdsS9RsTA"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>W[0,0,0:2]: </td>\n","    <td>[[-1.76806747  0.99336963 -0.33765555  0.73059859 -1.47724437][1.59544843 -2.28822502  0.49371023 -0.49784308  0.31660293]]</td>\n","  </tr>\n","  <tr>\n","    <td>b: </td>\n","    <td>[[[[ 0.75439794 -1.25286816 -0.51292982  0.29809284 -0.48851815]]]]</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"goYhkmioRQvI"},"source":["## 3.2 Maxpooling layer\n","\n","The pooling layer reduces the size (height and width) of the input. It helps reduce computation, as well as makes feature detectors more invariant to their position in the input.\n","\n","In this section, we will focus on Maxpooling layer. The implemented function will be gradually incorporated into this class, so you should use self.function() whenever you need to call it."]},{"cell_type":"markdown","metadata":{"id":"fK7eH6aGQo10"},"source":["### 3.2.1 Initialize the Maxpooling layer\n","\n","First, we initialize the Maxpooling layer.\n","\n","- It will take following parameters to initialize the Maxpooling layer:\n","\n","    *   pool_size: Defines the dimensions of the pooling window, which will be of shape (pool_size x pool_size).\n","    \n","    *   stride: The number of steps the pooling window moves during each operation.\n","\n","- Function **create_mask_from_window** is used in the backward pass to aid in backpropagating gradients through the pooling layer.\n","    \n","**Note: No implementation is required for this section.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yjg2Lh6qQo11"},"outputs":[],"source":["class MaxPool():\n","    def __init__(self, pool_size=2, stride=2):\n","\n","        self.pool_size = pool_size\n","        self.stride = stride\n","\n","    def create_mask_from_window(self, x):\n","        \"\"\"\n","        Creates a mask from an input x to identify the max entry of x.\n","\n","        Arguments:\n","        x -- Array of shape (filter_size, filter_size)\n","\n","        Returns:\n","        mask -- Array of the same shape as filter, contains a True at the position corresponding to the max entry of x.\n","        \"\"\"\n","\n","        mask = x == np.max(x)\n","\n","        return mask"]},{"cell_type":"markdown","metadata":{"id":"GqoB5MBSQo11"},"source":["### 3.2.2 Forward pass\n","\n","In this section, you will slide a ( pool_size * pool_size ) window over the input and store the max value of the window in the output.\n","\n","This involves the following steps:\n","\n","**Step 1: Calculate the Output Dimension**\n","   \n","- The output of the maxpooling operation is of shape (n_H, n_W, n_C):\n","\n","  - Height: n_H $= \\left\\lfloor \\frac{H_{prev} - pool\\_size}{\\text{stride}} \\right\\rfloor + 1$\n","\n","  - Width: n_W $= \\left\\lfloor \\frac{W_{prev} - pool\\_size}{\\text{stride}} \\right\\rfloor + 1$\n","\n","  - Depth: n_C_prev, which corresponds to the number of input channels\n","\n","**Step 2: Loop Through Training Examples**\n","\n","For each training example, follow these steps to perform maxpooling operation:\n","    \n","- **Step 2-1: Extracting slices**:\n","\n","  - This step is the same as what you have implemented in **Conv.forward**, but now we extract only one channel at a time to perform the max-pooling operation.\n","\n","- **Step 2-2: Applying Maxpooling**:\n","   \n","  - For each slice of shape (p, p , 1), calculate the maximum value from the slice.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WAX5OHXgQo11"},"outputs":[],"source":["def forward(self, A_prev):\n","    \"\"\"\n","    Implements the forward pass of the max pooling layer\n","\n","    Arguments:\n","    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n","\n","    Returns:\n","    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n","    \"\"\"\n","\n","    ### START CODE HERE ###\n","    # retrieve dimensions from the input shape\n","    (m, n_H_prev, n_W_prev, n_C_prev) = ...\n","\n","\n","    # Step 1: Output Dimension Calculation\n","    n_H = ...\n","    n_W = ...\n","    n_C = ...\n","\n","    # initialize output matrix A with zeros\n","    A = ...\n","\n","    # Step 2: Loop Through Training Examples\n","    for i in range(m):                           # loop over the batch of training examples\n","        for h in range(n_H):                     # loop on the vertical axis of the output volume\n","            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n","                for c in range (n_C):            # loop over the channels of the output volume\n","\n","                    # Step 2-1: Extracting slices\n","                    vert_start = ...\n","                    vert_end = ...\n","                    horiz_start = ...\n","                    horiz_end = ...\n","                    a_prev_slice = ...\n","\n","                    # Step 2-2: Applying Maxpooling\n","                    ...\n","\n","    ### END CODE HERE ###\n","\n","    # Store the input in \"cache\" for backward pass\n","    self.cache = A_prev\n","\n","    # Making sure your output shape is correct\n","    assert(A.shape == (m, n_H, n_W, n_C))\n","\n","    return A\n","\n","MaxPool.forward = forward"]},{"cell_type":"markdown","metadata":{"id":"3oW4gkFQQo11"},"source":["#### **Test and Evaluate** the **forward** function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BpL0HQvQRQvJ"},"outputs":[],"source":["np.random.seed(seed)\n","A_prev = np.random.randn(2, 4, 4, 3)\n","maxpool=MaxPool(pool_size=3, stride=2)\n","A = maxpool.forward(A_prev)\n","print(\"A =\", A)\n","\n","A_prev = np.random.randn(2, 5, 5, 3)\n","maxpool=MaxPool(pool_size=2, stride=1)\n","A = maxpool.forward(A_prev)\n","output[\"maxpool_forward\"] = A"]},{"cell_type":"markdown","metadata":{"id":"9vcEzFinVYHP"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>A: </td>\n","    <td>[[[[1.74481176 0.86540763 1.13376944]]]\n","\n","\n"," [[[1.13162939 1.51981682 2.18557541]]]]</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"Fznc1tR0Qo11"},"source":["### 3.2.3 Backward Pass\n","\n","In the backward pass, you will distribute the gradient from the output back to the input. The gradient will only pass through the location of the maximum value selected during the forward pass. During this process, the output gradient dA is propagated back to the input by identifying where the maximum value occurred in each pooling window.\n","\n","<span style=\"font-size: 25px;\">Backward Pass Steps</span>\n","\n","Given the gradient of the cost with respect to the output of the pooling layer, dA, which has the same shape as A, the backward pass proceeds as follows:\n","\n","**Step 1: Initialize Gradients**:\n","\n","   - Create dA_prev with the correct shapes\n","\n","**Step 2: Loop Through Training Examples**:\n","\n","   For each training example, follow these steps to pass through the gradients:\n","\n","   - **Step 3-1: Extracting slices**:\n","\n","     - This step is the same as what you have implemented in **Forward pass**.\n","\n","   - **Step 3-2: Pass through the Gradients**:\n","\n","      - call **create_mask_from_window** and apply the mask to the input gradient to backpropagate only the location of  maximum value within the window.\n","      \n","      Iterating over the height, width, and channels of the output to compute the full set of gradients.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dZbg-lrFQo12"},"outputs":[],"source":["def backward(self, dA):\n","    \"\"\"\n","    Implements the backward pass of the max pooling layer\n","\n","    Arguments:\n","    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n","\n","    Returns:\n","    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n","    \"\"\"\n","\n","    # Retrieve information from cache\n","    A_prev = self.cache\n","\n","    ### START CODE HERE ###\n","\n","    # Retrieve dimensions from A_prev's shape and dA's shape\n","    m, n_H_prev, n_W_prev, n_C_prev = ...\n","    m, n_H, n_W, n_C = ...\n","\n","    # Step 1: Initialize Gradients\n","    dA_prev = ...\n","\n","    # Step 2: Loop Through Training Examples\n","    for i in range(m):                           # loop over the batch of training examples\n","        for h in range(n_H):                     # loop on the vertical axis of the output volume\n","            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n","                for c in range (n_C):            # loop over the channels of the output volume\n","\n","                    # Step 2-1: Extracting slices\n","                    vert_start = ...\n","                    vert_end = ...\n","                    horiz_start = ...\n","                    horiz_end = ...\n","                    a_prev_slice = ...\n","\n","                    # Step 2-2: Pass through the Gradients\n","                    mask = ...\n","                    ...\n","\n","    ### END CODE HERE ###\n","\n","    # Make sure your output shape is correct\n","\n","    assert(dA_prev.shape == A_prev.shape)\n","\n","    return dA_prev\n","\n","MaxPool.backward = backward"]},{"cell_type":"markdown","metadata":{"id":"tgZ5GQK2Qo12"},"source":["#### **Test and Evaluate** the **backward** function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r023NZUnQo12"},"outputs":[],"source":["np.random.seed(seed)\n","dA = np.random.randn(2, 1, 1, 3)\n","maxpool = MaxPool(pool_size=2, stride=2)\n","maxpool.cache = np.random.randn(2, 2, 2, 3)\n","dA_prev = maxpool.backward(dA)\n","print(\"dA_prev[0] =\", dA_prev[0])\n","\n","dA = np.random.randn(2, 1, 1, 2)\n","maxpool = MaxPool(pool_size=3, stride=3)\n","maxpool.cache = np.random.randn(2, 3, 3, 2)\n","dA_prev = maxpool.backward(dA)\n","output[\"maxpool_backward\"] = dA_prev[0]"]},{"cell_type":"markdown","metadata":{"id":"JPg8sITPQo12"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>dA_prev[0]: </td>\n","    <td>[[[ 1.62434536  0.          0.        ][ 0.         -0.61175641  0.        ]]\n","\n"," [[ 0.          0.         -0.52817175]\n","  [ 0.          0.          0.        ]]]</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"Wn-VBGGURQvJ"},"source":["## 3.3 Flatten layer"]},{"cell_type":"markdown","metadata":{"id":"cJN7EvSuaKGW"},"source":["To connect the convolution layer and the dense layer, you should flatten the output of the convolution layer or max pooling layer before dense layer.\n","\n","In this section, we will focus on Flatten layer. The implemented function will be gradually incorporated into this class, so you should use self.function() whenever you need to call it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e1K19Y_SQo12"},"outputs":[],"source":["class Flatten():\n","    def __init__(self):\n","        pass"]},{"cell_type":"markdown","metadata":{"id":"5Haf0l4nau3y"},"source":["### 3.3.1 Forward pass\n","\n","The forward pass of the flatten layer converts each example in the input into a single row by flattening along the spatial dimensions.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5aw8ZJ9WQo13"},"outputs":[],"source":["def forward(self, A_prev):\n","    \"\"\"\n","    Implements the forward pass of the flatten layer\n","\n","    Arguments:\n","    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n","\n","    Returns:\n","    A -- output of the flatten layer, a 2-dimensional array of shape (m, (n_H_prev * n_W_prev * n_C_prev))\n","    \"\"\"\n","\n","    # Save information in \"cache\" for the backward pass\n","    self.cache = A_prev.shape\n","\n","    ### START CODE HERE ###\n","    A = ...\n","    ### END CODE HERE ###\n","    return A\n","\n","Flatten.forward = forward"]},{"cell_type":"markdown","metadata":{"id":"gj12447QQo13"},"source":["#### **Test and Evaluate** the **forward** function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TF96C0Fyat_4"},"outputs":[],"source":["np.random.seed(seed)\n","A_prev = np.random.randn(2,2,2,2)\n","flatten = Flatten()\n","A = flatten.forward(A_prev)\n","print(\"A.shape =\", A.shape)\n","print(\"A[0] =\", A[0])\n","\n","\n","np.random.seed(seed)\n","A_prev = np.random.randn(2,3,3,2)\n","flatten = Flatten()\n","A = flatten.forward(A_prev)\n","output[\"flatten_forward\"] = A[0]"]},{"cell_type":"markdown","metadata":{"id":"Rq3qbOjiVhjj"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>A.shape: </td>\n","    <td>(2, 8)</td>\n","  </tr>\n","  <tr>\n","    <td>A[0]: </td>\n","    <td>[ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.3015387\n","  1.74481176 -0.7612069 ]</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"qdWPIB6_a8_n"},"source":["### 3.3.2  Backward pass\n","\n","Here, we only need to reshape the input gradients back to their original dimensions (matching the output shape of the previous layer)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iL1MguUlQo13"},"outputs":[],"source":["def backward(self, dA):\n","    \"\"\"\n","    Implements the backward pass of the flatten layer\n","\n","    Arguments:\n","    dA -- Input data, a 2-dimensional array\n","\n","    Returns:\n","    dA_prev -- An array with its original shape (the output shape of its' previous layer).\n","    \"\"\"\n","    ### START CODE HERE ###\n","    dA_prev = ...\n","    ### END CODE HERE ###\n","    return dA_prev\n","\n","Flatten.backward = backward"]},{"cell_type":"markdown","metadata":{"id":"d6r3cHzTQo13"},"source":["#### **Test and Evaluate** the **backward** function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dY8vpJPLauWd"},"outputs":[],"source":["np.random.seed(seed)\n","A_prev = np.random.randn(2,2,2,2)\n","flatten = Flatten()\n","A = flatten.forward(A_prev)\n","B = flatten.backward(A)\n","print(\"B.shape =\", B.shape)\n","print(\"B[0] =\", B[0])\n","\n","# B and A_prev should be same\n","assert((B==A_prev).all())\n","\n","np.random.seed(seed)\n","A_prev = np.random.randn(4,3,3,3)\n","flatten = Flatten()\n","A = flatten.forward(A_prev)\n","B = flatten.backward(A)\n","output[\"flatten_backward\"] = B[0]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3khMiPehVjIV"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>B.shape: </td>\n","    <td>(2, 2, 2, 2)</td>\n","  </tr>\n","  <tr>\n","    <td>B[0]: </td>\n","    <td>[[[ 1.62434536 -0.61175641]\n","  [-0.52817175 -1.07296862]]\n","\n"," [[ 0.86540763 -2.3015387 ]\n","  [ 1.74481176 -0.7612069 ]]]</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"RYqpQu6Eye7h"},"source":["## 3.4 Model\n","Alright, now you have all the tools that are needed to build a convolutional neural network. Let's get started! Use the knowledge you learned from Lab4 to finish this part. But the method to build our model is slightly different:\n","\n","1. In this lab, we will call model.add( ) to add a layer into the model. For example:\n","    * model.add(Conv( )): add a convolution layer into the model.\n","    * model.add(Dense( )): add a dense layer into the model.\n","    * model.add(Activation( )): add an activation layer into the model.\n","\n","2. There’s no need to pass loss function parameters when defining the Activation layer.\n","    * You can simply call Activation('relu', None).\n","\n","In this section, we will use all the layers we've defined to build the model. You can refer to page 5 of the Lab 5 slides to see how a CNN stacks each layer together. There remains some functions to complete.\n","\n","- **1. forward**\n","\n","  - For each layer, call its forward function to compute the final output.\n","\n","- **2. backward**\n","\n","  - For the final ($L_{th}$) layer: Since we only perform binary classification in this lab, the last layer will always be a sigmoid activation. Refer to Lab4 for guidance on backpropagating through a sigmoid activation.\n","\n","  - For the remaining layers: Call each layer’s backward function to calculate gradients.\n","\n","- **3. update**\n","\n","  - For the Conv and Dense layers, call the update function to update the parameters.\n","\n","  **Hint:**\n","  - You can use layer.\\_\\_class\\_\\_.\\_\\_name\\_\\_ to obtain the name of each layer.\n","  - Remember to call the method of the layer inside the class, rather than calling it directly. For example, use self.layer.forward() instead of just forward().\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7dWrCCkPRQvK"},"outputs":[],"source":["class Model():\n","    def __init__(self):\n","        self.layers=[]\n","\n","    def add(self, layer):\n","        self.layers.append(layer)\n","\n","    def forward(self, X):\n","        A = X\n","\n","        ### START CODE HERE ###\n","        for l in range(len(self.layers)):\n","            ...\n","        ### END CODE HERE ###\n","        return A\n","\n","    def backward(self, AL=None, Y=None):\n","        L = len(self.layers)\n","\n","        ### START CODE HERE ###\n","        ...\n","\n","        # Loop from l=L-2 to l=0\n","        for l in reversed(range(L-1)):\n","            ...\n","        ### END CODE HERE ###\n","\n","        return dA_prev\n","\n","    def update(self, learning_rate):\n","\n","        # Only convolution layer and dense layer have to update parameters\n","        ### START CODE HERE ###\n","        for l in range(len(self.layers)):\n","          ...\n","        ### END CODE HERE ###\n"]},{"cell_type":"markdown","metadata":{"id":"36my0zWnlv3K"},"source":["#### **Test and Evaluate** the **Model** class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gN-8NQ_KRQvK"},"outputs":[],"source":["np.random.seed(seed)\n","A = np.random.randn(4,10,10,3)\n","Y = np.expand_dims(np.array([1,0,1,0]),-1)\n","\n","model=Model()\n","model.add(Conv(filter_size=3, input_channel=3, output_channel=8, pad=1, stride=2))\n","model.add(Activation(\"relu\", None))\n","model.add(MaxPool(pool_size=2, stride=2))\n","model.add(Flatten())\n","model.add(Dense(32, 1))\n","model.add(Activation(\"sigmoid\", None))\n","\n","\n","AL = model.forward(A)\n","dA_prev = model.backward(AL=AL, Y=Y)\n","model.update(0.01)\n","\n","print(\"model.layers[0].dW[0,0,0] =\", model.layers[0].dW[0,0,0])\n","print(\"model.layers[0].db =\", model.layers[0].db)\n","print(\"model.layers[4].dW[:8,0] =\", model.layers[4].dW[:8, 0])\n","print(\"model.layers[4].db =\", model.layers[4].db)\n","\n","\n","np.random.seed(seed)\n","A = np.random.randn(4,8,8,3)\n","Y = np.expand_dims(np.array([1,1,0,0]),-1)\n","\n","model=Model()\n","model.add(Conv(filter_size=3, input_channel=3, output_channel=16, pad=1, stride=2))\n","model.add(Activation(\"relu\", None))\n","model.add(MaxPool(pool_size=2, stride=2))\n","model.add(Flatten())\n","model.add(Dense(64, 1))\n","model.add(Activation(\"sigmoid\", None))\n","\n","\n","AL = model.forward(A)\n","dA_prev = model.backward(AL=AL, Y=Y)\n","model.update(0.001)\n","\n","output[\"model_1\"] = model.layers[0].dW[0,0,0]\n","output[\"model_2\"] = model.layers[0].db\n","output[\"model_3\"] = model.layers[4].dW[:8, 0]\n","output[\"model_4\"] = model.layers[4].db"]},{"cell_type":"markdown","metadata":{"id":"vGTtpnWcVvce"},"source":["Expected output:\n","<table>\n","  <tr>\n","    <td>model.layers[0].dW[0,0,0]: </td>\n","    <td>[ 0.09033835 -0.02115584 -0.00031401 -0.18961698 -0.02691661 -0.07641501\n"," -0.15402248  0.04322364]</td>\n","  </tr>\n","  <tr>\n","    <td>model.layers[0].db: </td>\n","    <td>[[[[-0.04353359 -0.29034244  0.55228045  0.27299323  0.27469552\n","    -0.24907673  0.55674122 -0.05243406]]]]</td>\n","  </tr>\n","  <tr>\n","    <td>model.layers[4].dW[:8,0]: </td>\n","    <td>[-2.14606176 -0.75085187 -1.19750975 -0.8916535  -0.91436404 -0.76753\n"," -1.30207298 -0.52670234]</td>\n","  </tr>\n","  <tr>\n","    <td>model.layers[4].db: </td>\n","    <td>[[-0.47493517]]</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"HkJeG7PHQo14"},"source":["# 4. Advanced part"]},{"cell_type":"markdown","metadata":{"id":"9EC0qy26RQvN"},"source":["Congratulations on implementing all the functions by yourself. You have done an incredible job! 👏\n","\n","Now you have all the tools you need to get started with classification. In this section, you will build a binary classifier using the functions you had previously written. You will create a model that can determine whether a chest X-ray image is normal or not. There will be 600 training images and 600 testing images, and the size of all images are 32 * 32 * 1.\n","\n","\n","- Implement a binary classifier and tune the hyperparameter.\n","\n","- You will receive 10% if your prediction accuracy exceeds 0.65 on the testing data and 20% if it exceeds 0.75."]},{"cell_type":"markdown","metadata":{"id":"K2X2fb7aoJTg"},"source":["## 4.1 Read the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTZ4JKCoQo14"},"outputs":[],"source":["# Use np.load to load the data from npz file\n","### START CODE HERE ###\n","X_train = ...\n","y_train = ...\n","X_test = ...\n","### END CODE HERE ###\n","\n","# plot first few images\n","for i in range(9):\n","    # define subplot\n","    plt.subplot(330 + 1 + i)\n","    # plot raw pixel data\n","    plt.imshow(X_train[i].squeeze(), cmap='gray', vmin=0, vmax=1)\n","    plt.title(y_train[i])\n","# show the figure\n","plt.show()\n","\n","# check the shape of training data and testing data\n","print('Train: X=%s, y=%s' % (X_train.shape, y_train.shape))\n","print('Test: X=%s' % (X_test.shape, ))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iwhza4gUboQ6"},"outputs":[],"source":["#You can split training and validation set here using train_test_split (Optional)\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"qdK9_gnZAjYD"},"source":["## 4.2 mini-batch gradient descent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LVTeqK9TqMwP"},"outputs":[],"source":["def random_mini_batches(X, Y, mini_batch_size = 64):\n","    \"\"\"\n","    Creates a list of random minibatches from (X, Y)\n","\n","    Arguments:\n","    X -- input data, of shape !!!!!!!!!!!(number of examples ,input size)!!!!!!!!!!!\n","    Y -- true \"label\" vector, of shape (number of examples, 1)\n","    mini_batch_size -- size of the mini-batches, integer\n","\n","    Returns:\n","    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n","    \"\"\"\n","\n","    m = X.shape[0]  # number of training examples\n","    mini_batches = []\n","\n","    # GRADED CODE: Binary classification\n","    ### START CODE HERE ###\n","\n","    # Step 1: Shuffle (X, Y)\n","    permutation = list(np.random.permutation(m))\n","    shuffled_X = ...\n","    shuffled_Y = ...\n","\n","    inc = mini_batch_size\n","\n","    # Step 2 - Partition (shuffled_X, shuffled_Y).\n","    # Cases with a complete mini batch size only i.e each of 64 examples.\n","    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n","    for k in range(0, num_complete_minibatches):\n","        mini_batch_X = ...\n","        mini_batch_Y = ...\n","        mini_batch = (mini_batch_X, mini_batch_Y)\n","        mini_batches.append(mini_batch)\n","\n","    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n","    if m % mini_batch_size != 0:\n","        mini_batch_X = ...\n","        mini_batch_Y = ...\n","        mini_batch = (mini_batch_X, mini_batch_Y)\n","        mini_batches.append(mini_batch)\n","\n","\n","    return mini_batches\n","\n","    ### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"5FIrnqYMFGRq"},"source":["## 4.3 Start training"]},{"cell_type":"markdown","metadata":{"id":"iMqcaNynQo15"},"source":["- Refer to page 5 of the Lab5 slides as a guide to build your model.\n","- Use ReLU as the activation function in the convolutional block instead of Sigmoid.\n","- Note that the final convolutional block doesn’t necessarily require a max-pooling layer.\n","\n","**Note:** If training takes too long, consider reducing the output dimension before the dense layer or increasing the batch size. For example, a dense layer with 10,000 neurons might take around 3 hours to train, while reducing this to 128 neurons or fewer could complete training in about 30 minutes. This is just an approximation—feel free to design the model according to your needs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1CBktduDyKd6"},"outputs":[],"source":["### START CODE HERE ###\n","learning_rate = 3e-5\n","num_iterations = 20\n","batch_size = 64\n","costs = []   # keep track of cost\n","\n","\n","# build the model\n","...\n","\n","# Loop (gradient descent)\n","for i in range(0, num_iterations):\n","    print(\"epoch: \",i)\n","    mini_batches = random_mini_batches(X_train, y_train, batch_size)\n","\n","    for batch in mini_batches:\n","        x_batch, y_batch = batch\n","\n","        # forward\n","        AL = ...\n","\n","        # compute cost\n","        cost = ...\n","\n","        # backward\n","        dA_prev = ...\n","\n","        # update\n","        ...\n","\n","    print (\"Cost after iteration %i: %f\" %(i, cost))\n","    costs.append(cost)\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"G-MWR00BQo15"},"source":["## 4.4 Evaluate your model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wzGHGrASQo15"},"outputs":[],"source":["### START CODE HERE ###\n","plt.plot(np.squeeze(costs))\n","plt.ylabel('cost')\n","plt.xlabel('iterations (per hundreds)')\n","plt.title(\"Learning rate =\" + str(learning_rate))\n","plt.show()\n","\n","print('training------')\n","pred_train = predict(model, X_train, y_train)\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"nrasoS0gQo15"},"source":["## 4.5 Generate the prediction\n","- Remember to submit this to Kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wKcWJUu8Qo15"},"outputs":[],"source":["pred_test = predict(model, X_test)\n","df = pd.DataFrame({\n","    'ID': range(len(X_test)),\n","    'Label': pred_test.astype(int).flatten()\n","})\n","df.to_csv('Lab5_prediction.csv', index=False, mode='w')"]},{"cell_type":"markdown","metadata":{"id":"WXGnS3HQeNUc"},"source":["# 5. Generate Lab5_output.npy\n","- Remember to submit this to eeclass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"twMsmXbQeDL_"},"outputs":[],"source":["# sanity check\n","assert(list(output.keys()) == ['zero_padding', 'conv_single_step', 'conv_forward_1', 'conv_forward_2', 'conv_forward_3','conv_backward_1', 'conv_backward_2', 'conv_backward_3', 'conv_update_1', 'conv_update_2', 'maxpool_forward', 'maxpool_backward', 'flatten_forward', 'flatten_backward', 'model_1', 'model_2', 'model_3', 'model_4'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bCJ0XTO_zE8A"},"outputs":[],"source":["np.save(\"Lab5_output.npy\", output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wFBFUUEg1to-"},"outputs":[],"source":["# sanity check\n","submit = np.load(\"Lab5_output.npy\", allow_pickle=True).item()\n","for key, value in submit.items():\n","    print(str(key) + \": \" + str(type(value)))"]},{"cell_type":"markdown","metadata":{"id":"MBkBtZHxIh8Z"},"source":["Expected output:<br>\n","<small>\n","zero_padding： <class 'numpy.ndarray'> <br>\n","conv_single_step： <class 'numpy.float64'> <br>\n","conv_forward_1： <class 'tuple'> <br>\n","conv_forward_2： <class 'numpy.float64'> <br>\n","conv_forward_3： <class 'numpy.ndarray'> <br>\n","conv_backward_1： <class 'tuple'> <br>\n","conv_backward_2： <class 'numpy.float64'> <br>\n","conv_backward_3： <class 'numpy.ndarray'> <br>\n","conv_update_1： <class 'numpy.ndarray'> <br>\n","conv_update_2： <class 'numpy.ndarray'> <br>\n","maxpool_forward： <class 'numpy.ndarray'> <br>\n","maxpool_backward： <class 'numpy.ndarray'> <br>\n","flatten_forward： <class 'numpy.ndarray'> <br>\n","flatten_backward： <class 'numpy.ndarray'> <br>\n","model_1： <class 'numpy.ndarray'> <br>\n","model_2： <class 'numpy.ndarray'> <br>\n","model_3： <class 'numpy.ndarray'> <br>\n","model_4： <class 'numpy.ndarray'> <br>\n","</small>"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"tf_2.6_py_3.7","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"}},"nbformat":4,"nbformat_minor":0}
